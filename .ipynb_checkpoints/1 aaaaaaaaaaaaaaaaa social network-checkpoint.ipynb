{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social networks\n",
    "Previously we labelled opinions of tweeters according to whether they were for or against the Mayor on the causes of serious violent crime and now we want to understand whether these opinions are spontaneous or closely related to the consensus opinion held within the social groups to which the tweeters belong.\n",
    "\n",
    "To do this we will first construct a network which captures all tweeter relationships and then interrogate this network to identify the communities that exist within it. Having done this we will then derive the majority opinion in these communities by summing the opinion of its individual members, whose opinions we had previously derived. We will then analyse those communities having a material difference in opinion to assess whether this difference reasonably represents a consensus view for the community. Our findings will inform our answers to the following research questions:\n",
    "- RQ1:  Can Twitter sentiment analysis determine the proportions of Twitter users that accept or reject the London Mayor’s evidence that ‘deprivation in the leading cause of youth violent crime in London’?\n",
    "- RQ2: Can this approach additionally identify the social groups to which users rejecting the Mayor’s evidence belong, and whether this view is widely spread within those groups?\n",
    "\n",
    "The reason we assess RQ1 in this notebook is because we previously concluded the classifier wasn’t a strong predictor of opinion within individual tweets but we had left open the question of whether it was a reasonable predictor of <b>tweeter opinion</b> in case the predictions improved by virtue of aggregating across many tweets. In effect, we are testing whether having greater volumes of data outweighs the shortcomings in the accuracy of our algorithm\n",
    "\n",
    "In summary, the objective for this notebook is to assess whether the tweeters holding predicted opinions are clustered within similar communities and whether this relationship is strong enough to confirm if our classifier is a good predictor of tweeter opinions. \n",
    "\n",
    "## Context\n",
    "(please refer to Appendix A for a glossary of terms used)\n",
    "\n",
    "We previously employed techniques similar to those used by Bovet et al [1] to label tweeter opinions and we will now apply a number of the techniques they used to identify the communities within our tweeter network, and to see which opinions they express. In their method they create communities through identifying the largest strong and weak components in the network (Strong Component Giant Component (SCGC) and Weak Component Giant Component (WCGC) respectively) and then analysed the proportion of tweeters in these components (or communities) expressing support for either Donald Trump or Hilary Clinton. They did this over a period of 6 months and, in particular, they analysed how SCGC and WCGC size and opinion proportions changed over time, in response to key events such as conventions and debates.\n",
    "\n",
    "Deriving the strong and weak components is one method for identifying communities but we will also employ the community detection algorithm used previously to identify communitees of hashtags in order to double check our results. As discussed previously, a community in a network can be broadly describes as a set of members (nodes/vertices) which have a greater density of links (edges) between them than they have with other communities or nodes within the network. The specific algorithm we will use is based on the work of Newman and Clauset, which has been implemented within the NetworkX library.\n",
    "\n",
    "While we can derive opinion proportions in the communities by aggregating the opinions of the tweeters belonging to them, we need to assess whether the aggregated opinions are an accurate reflection of the consensus opinion. This is because we do not have significant confidence in our opinion classifier. To do this we will derive centrality measures to identify the most important members in each community and then assess the opinions these members express on Twitter. For example, if tweeters A and B are considered the most central tweeters in a community we want to understand how closely their opinions coincide with consensus in the community. If there is a mismatch then we will have less confidence in the opinion deduced for the community.\n",
    "- The centrality measures we will use are degree, betweenness and eigenvector (see glossary)\n",
    "\n",
    "## Method\n",
    "On commencing this project we had intended to similarly assess how size and opinion mix in the SCGC and WCGC changed in response to key events such as the Mayor publishing the GLA report into the causes of serious violent crime and also during the London Mayoral election campaign, when crime was a key issue. However, Twitter API limitations meant we could only extract tweets from the previous week and we were not able to extract historic tweets at all. This meant we could only accumulate a months worth of data and this timeframe did not contain events significant enough for us to analyse changes in opinion over that timeframe. As a result, we can only be analyse opinion accumulated to a single point in time, specifically our latest extraction date. In addition, we know that our tweets were less polarized than those analysed by Bovet and so we want to understand whether they fragment in to a greater number of components. This means we will look at a number of the large components, not just the giant components.\n",
    "\n",
    "We can now discuss the specific steps we perform\n",
    "\n",
    "### 1. Create the interation network\n",
    "Load cleaned tweets and derive tweeter iteractions. \n",
    "- These interactions will comprise retweets, quote tweets, replies and mentions.\n",
    "\n",
    "Use NetworkX to construct the network using the interactions previously derived.\n",
    "- Nodes will be individual tweeters\n",
    "- Links (edges) will be their interactions\n",
    "\n",
    "The network will be a directed graph and so will capture bi-directional links.\n",
    "\n",
    "### 2. Derive centrality measures and add to network nodes\n",
    "- Derive degree, betweenness and eigenvector values and add to network as \n",
    "node attributes\n",
    "- Load tweeter opinions and assign to network as a node attribute. \n",
    "    - We first need to identify tweeters who have been retweeted but for whom we do not have original tweets in our dataset (and so we could not assign an opinion to them). Assign labels to them having manually reviewed the significant tweeters\n",
    "\n",
    "### 3. Derive Strong components\n",
    "- Using networkX, derive the strong components\n",
    "- Visualise and analyse the nodes within these components\n",
    "- Sum the opinion of each node in each component in order to derive the majority opinion within a component. \n",
    "- Compare consensus opinion with opinions expressed by most central tweeters in the component for each component that has a distinct consensus opinion. Discuss results.\n",
    "\n",
    "### 4. Derive Weak components\n",
    "- Using networkX, derive the weak components\n",
    "- Visualise and analyse the nodes within these components\n",
    "- Sum the opinion of each node in each component in order to derive the majority opinion within a component. \n",
    "- Compare consensus opinion with opinions expressed by most central tweeters in the component for each component that has a distinct consensus opinion. Discuss results.\n",
    "\n",
    "### 5. Derive communities using community detection\n",
    "- Using networkX, detect communities\n",
    "- Visualise and analyse the nodes within these communities\n",
    "- - Sum the opinion of each node in each community in order to derive the majority opinion within a community. \n",
    "- Compare consensus opinion with opinions expressed by most central tweeters in the community for each community that has a distinct consensus opinion. Discuss results.\n",
    "\n",
    "### 6. Conclusions\n",
    "We will discuss whether we could discern distinct communities with a clear consensus opinion and whether these are reliable, having checked the opinion of their key members. Having done this we will then provide answers to our research questions.\n",
    "\n",
    "\n",
    "## Notes \n",
    "- This is the final notebook and so the only output will be a discussion on how well the research questions have been answered, along with supporting materials.\n",
    "- Strongly connected networks reflect two way conversations but previously we saw 75% of our tweet corpus were retweets. It will be interesting to see the effect this has on our analysis.\n",
    "- Key sources\n",
    "    - The programminghistorian [5] provides an excellent primer for creating networks and deriving communities using community detection and we re-use his code extensively (we make explicit references in this notebook)\n",
    "    - We used bovet notebooks to inform the overall method [6], effectively as pseudo code and we more closely reused their code for identifying tweeter dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create the interation network\n",
    "### 1.1 Load cleaned tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_tweets = pd.read_csv(\"./DataSources/TwitterData/cleaned_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before after dropping rows with all NaN\n",
      "(41706, 26)\n",
      "(41706, 26)\n",
      "index                           41651\n",
      "tweet_id                        41651\n",
      "tweet_date                      41651\n",
      "tweeter_id                      41651\n",
      "tweeter_user_name               41651\n",
      "tweeter_screen_name             41651\n",
      "tweeter_location                41651\n",
      "message_text                    41651\n",
      "in_reply_to_user_screen_name     4607\n",
      "quote_tweet_screen_name           747\n",
      "favourite_count                 41651\n",
      "retweet_count                   41651\n",
      "is_retweet                      41651\n",
      "extract_run_date                41651\n",
      "retrieved_using_search_term     41651\n",
      "retweeted                       41651\n",
      "mentioned                       41651\n",
      "hashtags                        41651\n",
      "Tweet_punct                     41651\n",
      "Tweet_tokenized                 41651\n",
      "Tweet_nonstop                   41651\n",
      "Tweet_lemmatized                41651\n",
      "Clean_MessageText               41610\n",
      "tweet_date_dt                   41651\n",
      "time_bins_12h                   41651\n",
      "day_bins                        41651\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"shape before after dropping rows with all NaN\")\n",
    "print(all_tweets.shape)\n",
    "all_tweets = all_tweets.dropna(how='all') # only drops a row when every column is NA\n",
    "\n",
    "print(all_tweets.shape)\n",
    "\n",
    "# Now check for individual NaN values\n",
    "nan_values = all_tweets[all_tweets.isna().any(axis=1)]\n",
    "print(nan_values.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "- Set null values in_reply_to_user_screen_name and quote_tweet_screen_name fields to blanks because it's reasonable to have nothing in these fields\n",
    "- We know from prior notebooks that some tweets have null clean message text and so just set to blanks as we aren't looking at message text in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>tweeter_id</th>\n",
       "      <th>tweeter_user_name</th>\n",
       "      <th>tweeter_screen_name</th>\n",
       "      <th>tweeter_location</th>\n",
       "      <th>message_text</th>\n",
       "      <th>in_reply_to_user_screen_name</th>\n",
       "      <th>quote_tweet_screen_name</th>\n",
       "      <th>...</th>\n",
       "      <th>mentioned</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>Tweet_punct</th>\n",
       "      <th>Tweet_tokenized</th>\n",
       "      <th>Tweet_nonstop</th>\n",
       "      <th>Tweet_lemmatized</th>\n",
       "      <th>Clean_MessageText</th>\n",
       "      <th>tweet_date_dt</th>\n",
       "      <th>time_bins_12h</th>\n",
       "      <th>day_bins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index, tweet_id, tweet_date, tweeter_id, tweeter_user_name, tweeter_screen_name, tweeter_location, message_text, in_reply_to_user_screen_name, quote_tweet_screen_name, favourite_count, retweet_count, is_retweet, extract_run_date, retrieved_using_search_term, retweeted, mentioned, hashtags, Tweet_punct, Tweet_tokenized, Tweet_nonstop, Tweet_lemmatized, Clean_MessageText, tweet_date_dt, time_bins_12h, day_bins]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 26 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets.loc[all_tweets['in_reply_to_user_screen_name'].isna(), 'in_reply_to_user_screen_name'] = ''\n",
    "all_tweets.loc[all_tweets['quote_tweet_screen_name'].isna(), 'quote_tweet_screen_name'] = ''\n",
    "all_tweets.loc[all_tweets['Clean_MessageText'].isna(), 'Clean_MessageText'] = ''\n",
    "\n",
    "# Now check for individual NaN values\n",
    "nan_values = all_tweets[all_tweets.isna().any(axis=1)]\n",
    "\n",
    "nan_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Build interaction network\n",
    "There are four primary ways in which Twitter users interact: reply to, mention, retweet and quote tweet and we will create a body of interaction from our tweet dataset and use this to create our interation network. In this network, the tweeter will be vertices and the interaction between them will be the edges. Previously, when building our hashtag network, we used bespoke code to derive the strength on the edges by summing the number of times it occured. In this case we will use functions available within NetworkX to achieve the same goal.\n",
    "\n",
    "#### 1.2.1 Get all interactions\n",
    "For each tweet in cleaned_tweets, create a list of all tweeters that tweet interacts with.\n",
    "- Ignore tweets originating from the SadiqKhan or MayorOfLondon because we are not interested in their opinion and because they are too significant within the network.\n",
    "- The code used to derive the interations and then create the network (1.2.2) draws heavily on the work of Bovet [6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's define some functions to extract the interactions from tweets\n",
    "def string_to_list(my_str):\n",
    "    delimiter = \",\"\n",
    "    my_str = my_str.replace(\"[\", \"\")\n",
    "    my_str = my_str.replace(\"]\", \"\")\n",
    "    my_str = my_str.replace(\"@\", \"\")\n",
    "    my_str = my_str.replace(\"'\", \"\")\n",
    "    my_str = my_str.replace(\" \", \"\")\n",
    "    my_list = my_str.split(delimiter)\n",
    "    return my_list\n",
    "\n",
    "def getAllInteractions(tweet):\n",
    "    \n",
    "    # Get the tweeter\n",
    "    tweet_id = tweet.tweet_id\n",
    "    tweeter_id = tweet.tweeter_id\n",
    "    tweeter_name = tweet.tweeter_screen_name\n",
    "    \n",
    "    # a python set is a collection of unique items\n",
    "    # we use a set to avoid duplicated ids\n",
    "    interacting_users = set()\n",
    "    \n",
    "    # Add person they're replying to\n",
    "    if tweet.in_reply_to_user_screen_name != '':\n",
    "        interacting_users.add(tweet.in_reply_to_user_screen_name)\n",
    "        \n",
    "    # Add person they quoted\n",
    "    if tweet.quote_tweet_screen_name != '':\n",
    "        interacting_users.add(tweet.quote_tweet_screen_name)\n",
    "    \n",
    "    # Add person they retweeted\n",
    "    if len(tweet.retweeted) > 2: # because empty strings will contain []\n",
    "        retweeted_list = string_to_list(tweet.retweeted)\n",
    "        for item in retweeted_list:\n",
    "            interacting_users.add(item)\n",
    "       \n",
    "    # Add mentions\n",
    "    if len(tweet.mentioned) > 2: # because empty strings will contain []\n",
    "        mentioned_list = string_to_list(tweet.mentioned)\n",
    "        for item in mentioned_list:\n",
    "            interacting_users.add(item)\n",
    "  \n",
    "    # remove the tweeter if he is in the set\n",
    "    interacting_users.discard(tweeter_name)\n",
    "    \n",
    "    # Return our tweeter and their influencers\n",
    "    return tweeter_id, tweeter_name, tweet_id, list(interacting_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Use NetworkX to construct the social network.\n",
    "Previously, when building our hashtag network, we used bespoke code to derive the strength on the edges by summing the number of times it occured. In this case we will use functions available within NetworkX to achieve the same goal.\n",
    "- Define an empty Directed Graph: A directed graph is a graph where edges have a direction. In our case the edges goes from user that sent the tweet to the user with whom they interacted (retweeted, mentioned or quoted)\n",
    "- Create a set of all unique interacting users for later\n",
    "    - This is because we know we have tweeters who have been retweeted but haven't tweeted themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.DiGraph()\n",
    "\n",
    "all_interacting_users = []\n",
    "\n",
    "for index, tweet in all_tweets.iterrows():\n",
    "    \n",
    "    if (tweet.tweeter_screen_name != 'SadiqKhan') & (tweet.tweeter_screen_name != 'MayorofLondon'):\n",
    "        tweeter_id, tweeter_name, tweet_id, interactions = getAllInteractions(tweet)\n",
    "        \n",
    "        all_interacting_users.extend(interactions)\n",
    "    \n",
    "        # add an edge to the Graph for each influencer\n",
    "        for interact_name in interactions:\n",
    "            \n",
    "            # add edges between the two user ids\n",
    "            # this will create new nodes if the nodes are not already in the network\n",
    "            # we also add an attribute the to edge equal to the id of the tweet\n",
    "            G.add_edge(tweeter_name, interact_name, tweet_id=tweet_id)\n",
    "        \n",
    "            # add name as a property to each node\n",
    "            # with networkX each node is a dictionary\n",
    "            G.nodes[tweeter_name]['name'] = tweeter_name\n",
    "            G.nodes[interact_name]['name'] = interact_name\n",
    "            \n",
    "all_interacting_users_set = set(all_interacting_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Analyse high level network statistics\n",
    "We want to understand the number of nodes, the number of edges and the density of the network. \n",
    "- The following code was inspired by an online tutorial presented by Ladd et al [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tweeters: 23,857\n",
      "\n",
      "Name: \n",
      "Type: DiGraph\n",
      "Number of nodes: 26691\n",
      "Number of edges: 43666\n",
      "Average in degree:   1.6360\n",
      "Average out degree:   1.6360\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique tweeters: {:,}\\n\".format(all_tweets.tweeter_screen_name.nunique()))\n",
    "\n",
    "print(nx.info(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network density: 6.129569749993539e-05\n"
     ]
    }
   ],
   "source": [
    "# On a scale of 0 to 1, where 1 is a dense network\n",
    "density = nx.density(G)\n",
    "print(\"Network density:\", density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3.1 Illustrate node characteristics\n",
    "The keys are the tweeter_screen_names and we have also added this as a node attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['myerschrismyer1', 'LeslieH24367191', 'emilysheffield', 'standardnews', 'NaheedMajeed', 'basilewitch', 'Short2Cjs', 'DeanCowcher', 'sonya_annie', 'OliverSteinme16']\n",
      "\n",
      "\n",
      "<class 'dict'>\n",
      "{'name': 'standardnews'}\n"
     ]
    }
   ],
   "source": [
    "# the keys are the user_id\n",
    "nodelist = list(G.nodes.keys())\n",
    "print(nodelist[:10])\n",
    "print(\"\\n\")\n",
    "# each node is itself a dictionary with node attributes as key,value pairs\n",
    "print(type(G.nodes[nodelist[3]]))\n",
    "print(G.nodes[nodelist[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "- We can see that we have less than double the edges than nodes and this means we have an average degree of 1.63, meaning each node has less than 2 edges entering or departing. This will be useful to know when comparing with nodes which have a high degree centrality.\n",
    "- Network density is the ratio of actual edges in a network versus the total amount of all potential edges, where this would be the sum of edges in a network where every node connects to every other node. As discussed by Ladd, the density of a network represents how closely knit the nodes are, and density ranges between 0 and 1, with 1 representing a fully dense network. As can be seen, out network denisty measure is much nearer 0 and so this is a relatively sparse network (this confirms what we see with the degree measure).\n",
    "- Note the difference between the number of unique tweeters and the number of nodes created (one node for each tweeter screen name). We can see we have more nodes than original tweeters and this difference arises from interactions with tweeters (e.g. retweets) for whom we don't have the original tweet, because it was tweeted prior to when we started capturing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Derive centrality measures and add to network nodes\n",
    "### 2.1. Derive degree, betweenness and eigenvector values\n",
    "Then add to network as node attributes.\n",
    "\n",
    "#### 2.1.2 Degree\n",
    "Derive in and out degree and compare to see if nodes with many out edges also have many in edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "degree_dict = dict(G.degree(G.nodes()))\n",
    "nx.set_node_attributes(G, degree_dict, 'degree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_in_dict = dict(G.in_degree(G.nodes()))\n",
    "nx.set_node_attributes(G, degree_in_dict, 'in_degree')\n",
    "\n",
    "degree_out_dict = dict(G.out_degree(G.nodes()))\n",
    "nx.set_node_attributes(G, degree_out_dict, 'out_degree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from networkx.algorithms import community \n",
    "\n",
    "def print_sorted_degree(degree_list, n, description):\n",
    "    print(\"\\n<------------- {} ------------->\\n\".format(description))\n",
    "\n",
    "    for d in degree_list[:n]:\n",
    "        print(d)\n",
    "\n",
    "sorted_degree = sorted(degree_in_dict.items(), key=itemgetter(1), reverse=True)\n",
    "sorted_in_degree = sorted(degree_in_dict.items(), key=itemgetter(1), reverse=True)\n",
    "sorted_out_degree = sorted(degree_out_dict.items(), key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.2.1 Compare degree measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<------------- Top 20 nodes by degree: ------------->\n",
      "\n",
      "('SadiqKhan', 1296)\n",
      "('MayorofLondon', 933)\n",
      "('KoolKat1025', 912)\n",
      "('GBNEWS', 875)\n",
      "('James_Thorburn', 853)\n",
      "('NKrankie', 739)\n",
      "('LeaveEUOfficial', 737)\n",
      "('LeoKearse', 635)\n",
      "('LeslieH24367191', 563)\n",
      "('MartinDaubney', 528)\n",
      "('standardnews', 520)\n",
      "('polblonde', 489)\n",
      "('2tweetaboutit', 488)\n",
      "('metpoliceuk', 469)\n",
      "('LozzaFox', 444)\n",
      "('BrexitBassist', 433)\n",
      "('NormanBrennan', 383)\n",
      "('MickeyD44314901', 378)\n",
      "('Independent', 376)\n",
      "('PoliticsForAlI', 364)\n",
      "\n",
      "<------------- Top 20 nodes by IN degree: ------------->\n",
      "\n",
      "('SadiqKhan', 1296)\n",
      "('MayorofLondon', 933)\n",
      "('KoolKat1025', 912)\n",
      "('GBNEWS', 875)\n",
      "('James_Thorburn', 853)\n",
      "('NKrankie', 739)\n",
      "('LeaveEUOfficial', 737)\n",
      "('LeoKearse', 635)\n",
      "('LeslieH24367191', 563)\n",
      "('MartinDaubney', 528)\n",
      "('standardnews', 520)\n",
      "('polblonde', 489)\n",
      "('2tweetaboutit', 488)\n",
      "('metpoliceuk', 469)\n",
      "('LozzaFox', 444)\n",
      "('BrexitBassist', 433)\n",
      "('NormanBrennan', 383)\n",
      "('MickeyD44314901', 378)\n",
      "('Independent', 376)\n",
      "('PoliticsForAlI', 364)\n",
      "\n",
      "<------------- Top 20 nodes by OUT degree: ------------->\n",
      "\n",
      "('SilvertownTn', 99)\n",
      "('GillJames54', 66)\n",
      "('LianeCorinna', 48)\n",
      "('miroirdufou', 46)\n",
      "('Christi49034502', 43)\n",
      "('Yeah_ThatBloke2', 40)\n",
      "('TflTruth', 35)\n",
      "('RobertMadeley01', 34)\n",
      "('MarieM56091012', 33)\n",
      "('PatriciaNiclas', 32)\n",
      "('HippsLouis', 30)\n",
      "('rospay15', 27)\n",
      "('FJEB88', 27)\n",
      "('thegingerpig', 26)\n",
      "('PaulCra33107260', 26)\n",
      "('ToxicLemon69', 26)\n",
      "('bonnie_london', 26)\n",
      "('PCisfinished', 25)\n",
      "('wardj15', 25)\n",
      "('FleetSusie', 25)\n"
     ]
    }
   ],
   "source": [
    "print_sorted_degree(sorted_degree, 20, \"Top 20 nodes by degree:\")\n",
    "print_sorted_degree(sorted_in_degree, 20, \"Top 20 nodes by IN degree:\")\n",
    "print_sorted_degree(sorted_out_degree, 20, \"Top 20 nodes by OUT degree:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comments\n",
    "The IN degree has a far bigger influence on overall degree and the top members by in degree are quite different to the top members ranked by OUT degree. Let's confirm this by checking the OUT degree for the top members of the IN degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = list(list(zip(*sorted_degree[:10]))[0])\n",
    "for name in name_list:\n",
    "    print(\"{}: {}\".format(name,degree_out_dict[name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comments\n",
    "The OUT degree of the most important users by IN degree is low and this suggests the users with the highest overall degree are being tweeted at, rather than originating tweets themselves (the exception to this are SadiqKhan and MayorofLondon because we specifically excluded them as tweeters). In effect, their importance is being magnified by others amplifying what they have said. This appears to concord with what we found previously, namely that nearly 75% of our body of tweets were retweets, rather than original tweets. This is interesting when we consider further measures for node importance, namely betweenness and eigenvector. This is because eigenvector is calculated using the IN edges on a graph when derived for a directed graph, and this makes it a particularly important measure in our case because the IN edges are more important \n",
    "- \"For directed graphs this is “left” eigenvector centrality which corresponds to the in-edges in the graph. For out-edges eigenvector centrality first reverse the graph with G.reverse()\" https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.eigenvector_centrality.html  \n",
    "\n",
    "#### 2.1.3 Eigenvector and betweeness\n",
    "Derive eigenvector and betweeness measures and add as node attributes\n",
    "- This code was inspired by [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "betweenness_dict = nx.betweenness_centrality(G) # Run betweenness centrality\n",
    "eigenvector_dict = nx.eigenvector_centrality(G) # Run eigenvector centrality\n",
    "\n",
    "nx.set_node_attributes(G, betweenness_dict, 'betweenness')\n",
    "nx.set_node_attributes(G, eigenvector_dict, 'eigenvector')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.3.1 Analyse nodes with greatest betweeness and eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sorted_betweenness = sorted(betweenness_dict.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "#First get the top 20 nodes by betweenness as a list\n",
    "top_betweenness = sorted_betweenness[:20]\n",
    "\n",
    "#Then find and print their degree\n",
    "for tb in top_betweenness: # Loop through top_betweenness\n",
    "    degree = degree_dict[tb[0]] # Use degree_dict to access a node's degree\n",
    "    print(\"Name:\", tb[0], \"| Betweenness Centrality:\", tb[1], \"| Degree:\", degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_eigenvector = sorted(eigenvector_dict.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "#First get the top 20 nodes by eigenvector as a list\n",
    "top_eigenvector = sorted_eigenvector[:20]\n",
    "\n",
    "#Then find and print their degree\n",
    "for tb in top_eigenvector: # Loop through top_betweenness\n",
    "    degree = degree_dict[tb[0]] # Use degree_dict to access a node's degree\n",
    "    print(\"Name:\", tb[0], \"| Eigenvector Centrality:\", tb[1], \"| Degree:\", degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "There is a significant difference between eignevector and degree and this normally occurs when low scoring nodes are linked to lots of other low scoring nodes and so, while their degree is high, their importance is relatively low. Likewise, a node could have a high betweenness but a low eigenvector when the node is connecting lots of unimportant clusters. This combination of measures suggests a more fragmented network but we will assess this further below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Load tweeter opinions and add to network as a node attribute\n",
    "#### 2.2.1 Load opinions and review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_users = pd.read_csv('./DataSources/TwitterData/labeled_users.csv')\n",
    "print(labeled_users.shape)\n",
    "labeled_users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tweeters = all_tweets.tweeter_screen_name.unique()\n",
    "\n",
    "unique_tweeters_df = pd.DataFrame(unique_tweeters, columns =['tweeter_screen_name'])\n",
    "\n",
    "all_tweeters = pd.merge(unique_tweeters_df, labeled_users, how='left', left_on='tweeter_screen_name', \n",
    "                        right_on='tweeter_screen_name')\n",
    "\n",
    "all_tweeters.loc[all_tweeters['tweeter_label'].isna(), 'tweeter_label'] = 'NONE'\n",
    "\n",
    "display(all_tweeters.shape)\n",
    "all_tweeters.tweeter_label.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Labeling tweeters who haven't tweeted but were retweeted\n",
    "We have an issue because we have approximately 4,500 tweeters who have been retweeted but for whom we do not have original tweets in our dataset and so we could not assign an opinion to them. \n",
    "- Previously we created 'all_interacting_users_set' and now we use that as the basis for identifying users that were retweeted but didn't tweet, so that we can then review and asign them labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tweeters_set = set(unique_tweeters)\n",
    "print(\"Number of tweeters who have tweeted at least once: {:,}\".format(len(unique_tweeters)))\n",
    "print(\"Number of tweeters who haven't been retweeted: {:,}\".format(len(unique_tweeters_set.difference(all_interacting_users_set))))\n",
    "\n",
    "retweeted_not_tweeted = all_interacting_users_set.difference(unique_tweeters_set)\n",
    "print(\"Number of tweeters who have been retweeted but haven't tweeted: {:,}\".format(len(retweeted_not_tweeted)))\n",
    "\n",
    "retweeted_not_tweeted_list = list(retweeted_not_tweeted)\n",
    "\n",
    "df_all_interacting_users = pd.DataFrame(all_interacting_users, columns=['tweeter_screen_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2.1 Print tweeters with most interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_interacting_users_agg = df_all_interacting_users.groupby([\"tweeter_screen_name\"]).apply(lambda x: x['tweeter_screen_name'].count()).reset_index()\n",
    "df_all_interacting_users_agg.rename(columns = {0:'count'}, inplace = True)\n",
    "\n",
    "df_all_interacting_users_agg.sort_values(by='count', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2.2 Print tweeters who haven't tweeted but are most often interacted with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retweeted_not_tweeted = pd.DataFrame(retweeted_not_tweeted_list, columns=['tweeter_screen_name'])\n",
    "\n",
    "df_retweeted_not_tweeted_counts = pd.merge(df_retweeted_not_tweeted, df_all_interacting_users_agg, \n",
    "                                           how='inner', left_on='tweeter_screen_name', right_on='tweeter_screen_name')\n",
    "\n",
    "df_retweeted_not_tweeted_counts.sort_values(by='count', ascending=False)[:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comments\n",
    "The subset of users aren't among those who've been retweeted the most (e.g. MayorOfLondon retweeted 1,548 vs Keir_Starmer retweeted 198 times) and, of this smaller group, many of those who were retweeted most often are public figures and so just set their opinion to 'NONE' i.e. neutral. We do run the risk of mis labeling some users who've been retweeted but, for the reasons we've just laid out, we don't believe this will be material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retweeted_not_tweeted['tweeter_label'] = 'NONE'\n",
    "retweeted_not_tweeted_dict = dict(zip(df_retweeted_not_tweeted.tweeter_screen_name,df_retweeted_not_tweeted.tweeter_label))\n",
    "\n",
    "opinions_dict = dict(zip(all_tweeters.tweeter_screen_name,all_tweeters.tweeter_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from https://www.geeksforgeeks.org/python-merging-two-dictionaries/\n",
    "def Merge(dict1, dict2):\n",
    "    res = {**dict1, **dict2}\n",
    "    return res\n",
    "\n",
    "\n",
    "combined_opinions_dict = Merge(opinions_dict, retweeted_not_tweeted_dict)\n",
    "len(combined_opinions_dict), len(opinions_dict), len(retweeted_not_tweeted_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Assign opinion as a node attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_node_attributes(G, combined_opinions_dict, 'opinions')\n",
    "\n",
    "print(G.nodes[nodelist[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Derive Strong components\n",
    "\n",
    "### 3.1. Get strong components\n",
    "Using networkX, derive the strong components, namely the largest subgraphs containing nodes with bidirectional links.\n",
    "- This code draws heavily on code developed by Bovet [6] and networkX example documentation [7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this returns a list of set of nodes belonging to the \n",
    "# different (strongly) connected components\n",
    "components_strong = list(nx.strongly_connected_components(G))\n",
    "\n",
    "# sort the component according to their size\n",
    "components_strong = list(sorted(components_strong, key=lambda x:len(x), reverse=True))\n",
    "\n",
    "# make a list with the size of each component\n",
    "comp_sizes_strong = []\n",
    "for comp in components_strong:\n",
    "    comp_sizes_strong.append(len(comp))\n",
    "    \n",
    "print(\"number of strong components: {}\".format(len(comp_sizes_strong)))\n",
    "\n",
    "print(\"sizes of the ten largest components: {}\".format(comp_sizes_strong[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "These are very small strong components, for example in Bovet's research the number of nodes in their SCGC ranged between 1,500 and 5,000 nodes whereas our largest strong component has just 37 nodes.\n",
    "\n",
    "### 3.2. Visualise and discuss the strong components\n",
    "We will draw the subgraphs for each of the top 5 strong components and color the nodes according to their designated opinion. We will also display the tweeters in the components with the highest eignenvector centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "\n",
    "def color_lookup(color_key, mapping):\n",
    "    label = mapping[color_key]\n",
    "    return label\n",
    "\n",
    "def draw_strongly_connected(idx, components_strong):\n",
    "\n",
    "    largest_comp_strong = components_strong[idx]\n",
    "    LCC_strong = G.subgraph(largest_comp_strong)\n",
    "    \n",
    "    # now set colors\n",
    "    opinions = set(nx.get_node_attributes(LCC_strong,'opinions').values())\n",
    "    mapping = dict(zip(sorted(opinions),count()))\n",
    "    \n",
    "    nodes = LCC_strong.nodes()\n",
    "    colors = [mapping[LCC_strong.nodes[n]['opinions']] for n in nodes]\n",
    "    \n",
    "    colors_df = pd.DataFrame(colors, columns=['color'])\n",
    "    colors_agg_df = colors_df.groupby([\"color\"]).apply(lambda x: x['color'].count()).reset_index()\n",
    "    colors_agg_df.rename(columns = {0:'count'}, inplace = True)\n",
    "    \n",
    "    # need to swap keys and values - https://www.geeksforgeeks.org/python-program-to-swap-keys-and-values-in-dictionary/\n",
    "    new_mapping = dict([(value, key) for key, value in mapping.items()])\n",
    "       \n",
    "    colors_agg_df['label'] = ''\n",
    "    colors_agg_df['label'] = colors_agg_df.apply(lambda x: color_lookup(x['color'], new_mapping), axis=1)\n",
    "           \n",
    "    ## <--- now look at centrality\n",
    "    eigenValues = nx.eigenvector_centrality(LCC_strong)    \n",
    "       \n",
    "    print(\"\\n <---- The nodes with the highest eigenvalues are as follows ----> \\n\")  \n",
    "    \n",
    "    for key, value in sorted(eigenValues.items(),\n",
    "                        key=lambda item: item[1], reverse=True)[:10]:\n",
    "        \n",
    "        opinion = LCC_strong.nodes[key]['opinions']\n",
    "        degree = LCC_strong.nodes[key]['degree']\n",
    "        print(\"Node: {}, Eigenvalue: {}, opinion: {}, degree: {}\".format(key, value, opinion, degree))\n",
    "    \n",
    "\n",
    "    \n",
    "    display(\"Number of components in LCC[{}] = {}\".format(idx, LCC_strong.number_of_nodes()))\n",
    "    display(colors_agg_df[['label', 'count']].sort_values(by='count', ascending=False))\n",
    "    \n",
    "    pos = nx.spring_layout(LCC_strong)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.axis('off')\n",
    "\n",
    "    nx.draw_networkx(LCC_strong, pos, nodelist=nodes, node_color=colors, \n",
    "                            with_labels=False, node_size=300, cmap=plt.cm.jet)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "range_limit = 5\n",
    "for n in range(range_limit):\n",
    "    draw_strongly_connected(n, components_strong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "Components 1 and 4 are more related to traffic/pollution than crime and so we can ignore. We can ignore component 2, because while this group discusses a range of subjects, they aren't related to crime. This leaves components 0 and 3, but component 0 is by far our most interesting component, not least because it is our 'giant' component.\n",
    "- The majority of the top 10 tweeters can be seen to be anti mayor by virtue of what they tweet on Twitter. So what we can tell is that they interact with each other, because they exist within the same strong component, but we also tell that they are prolific retweeters of other tweeter because their degree value is relatively high compared with the rest of the network\n",
    "    - We know degree is more heavily influenced by IN degree than out degree, which in turn means they retweet more than they tweet\n",
    "    \n",
    "Unfortunately, the majority of this component have been labeled as NOT_AGAINST, when we know they lean AGAINST and so this suggests our opinion classifier is a poor predictor of opinion. We will confirm this by looking at the weak components and the detected communities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Derive weakly connected components\n",
    "\n",
    "### 4.1. Get weak components\n",
    "Using networkX, derive the weak components, namely the largest subgraphs containing nodes with links in one direction.\n",
    "- Also assign weak component id as a network attribute\n",
    "- This code draws heavily on code developed by Bovet [6] and networkX example documentation [7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "components = list(nx.weakly_connected_components(G))\n",
    "\n",
    "# sort the component according to their size\n",
    "components = list(sorted(components, key=lambda x:len(x), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_dict = {} # Create a blank dictionary\n",
    "for i,c in enumerate(components): # Loop through the list of communities, keeping track of the number for the community\n",
    "    for name in c: # Loop through each person in a community\n",
    "        components_dict[name] = i # Create an entry in the dictionary for the person, where the value is which group they belong to.\n",
    "\n",
    "# Now you can add modularity information like we did the other metrics\n",
    "nx.set_node_attributes(G, components_dict, 'weak_component_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 View largest weak components by size (number of nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_components_dict = pd.DataFrame(list(components_dict.items()),columns = ['screen_name','weak_comp_class_id']) \n",
    "comp_class_list = df_components_dict.weak_comp_class_id.unique()\n",
    "\n",
    "print(\"Number of different weak components = {} \".format(len(comp_class_list)))\n",
    "\n",
    "df_components_dict_agg = df_components_dict.groupby('weak_comp_class_id').count()\n",
    "\n",
    "comm_count = 10\n",
    "print(\"\\nTop {} components, by number of members\".format(comm_count))\n",
    "df_components_dict_agg.head(comm_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Analyse key attributes for biggest weak components\n",
    "THere are 1,535 different weak components but the vast majority are immaterial and we can see that component 0, by far the biggest component, contains nearly 80% of all nodes (20,961 out of a total of 26,691 nodes). \n",
    "\n",
    "We will now analyse the top 5 components to understand how opinions are expressed across these nodes. \n",
    "- We create a dataframe with node attributes to facilitate this analyis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_attributes = dict(G.nodes())\n",
    "node_attribute_values = node_attributes.values()\n",
    "\n",
    "node_attributes_df = pd.DataFrame(node_attribute_values,\n",
    "                      columns =['name', \n",
    "                                'degree',\n",
    "                                'betweenness',\n",
    "                                'eigenvector',\n",
    "                                'opinions',\n",
    "                                'weak_component_id'])\n",
    "\n",
    "print(\"Number of unique tweeters: {:,}\".format(node_attributes_df['name'].count()))\n",
    "\n",
    "node_attributes_df.rename(columns = {'name':'screen_name'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_list = [0,1,2,3,4]\n",
    "\n",
    "grouped_weak_component = node_attributes_df.groupby(['weak_component_id', 'opinions']).agg({'screen_name': ['count']})\n",
    "grouped_weak_component.columns = ['count']\n",
    "grouped_weak_component = grouped_weak_component.reset_index()\n",
    "grouped_weak_component = grouped_weak_component[grouped_weak_component.weak_component_id.isin(component_list)]\n",
    "grouped_weak_component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Create a treemap to analyse proportion of tweeters AGAINST and NOT AGAINST within our largest weak components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.treemap(grouped_weak_component, path=['weak_component_id', 'opinions'], values='count', color='opinions',\n",
    "                 color_discrete_map={'AGAINST':'rgb(104,132,236)', 'NOT_AGAINST':'rgb(209,101,92)', \n",
    "                                      'FOR':'yellow', \n",
    "                                      'None':'lightgrey'},\n",
    "                title=\"Top 6 weak components: proportions AGAINST, NOT_AGAINST, FOR, NONE\")\n",
    "# this is what I don't like, accessing traces like this\n",
    "fig.data[0].textinfo = 'label+text+value+percent parent+percent root'\n",
    "\n",
    "fig.layout.hovermode = False\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(grouped_weak_component, path=['opinions', 'weak_component_id'], values='count', color='opinions',\n",
    "                 color_discrete_map={'AGAINST':'rgb(104,132,236)', 'NOT_AGAINST':'rgb(209,101,92)', \n",
    "                                      'FOR':'yellow', \n",
    "                                      'None':'lightgrey'},\n",
    "                title=\"Labels AGAINST, NOT_AGAINST, FOR, NONE across weak components\")\n",
    "# this is what I don't like, accessing traces like this\n",
    "fig.data[0].textinfo = 'label+text+value+percent parent+percent root'\n",
    "\n",
    "fig.layout.hovermode = False\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "Component 0 contains by far the majority of nodes and so we will focus on it.\n",
    "- NOT_AGAINST nodes comprise around 55% of all nodes in component 0, while AGAINST comprises 32%. If we also consider the 12% with no label we can say that the biggest component largely expresses no opinion on the causes of crime. THis is broadly in keeping with the percentage of AGAINST nodes across the entire tweet corpus, which we calculated in notebook 1_10 (AGAINST => 8,226/23,108 => 36%)\n",
    "- We can also see that 97% of all nodes designated with opinion of AGAINST reside in component 0 (strictly speaking this is 97% of all nodes across the top 5 weak components, which in turn represent 80% of all nodes)\n",
    "\n",
    "This means that component 0 is by far our most important node.\n",
    "\n",
    "### 4.3. Visualise and discuss the weak components\n",
    "We will draw the subgraphs for each of the top 5 weak components and color the nodes according to their designated opinion. We will also display the tweeters in the components with the highest eignenvector centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_weak_components(n, components, G):\n",
    "    # let's make a new graph which is the subgraph of G corresponding to index=n\n",
    "    \n",
    "    print(\"\\n<------------- subcomponent: {} --------------->\\n\".format(n))\n",
    "    \n",
    "    \n",
    "    # First get a list of just the nodes in that class\n",
    "    comp_id = [i for i in G.nodes() if G.nodes[i]['weak_component_id'] == n]\n",
    "\n",
    "    # Then create a dictionary of the eigenvector centralities of those nodes\n",
    "    comp_eigenvector = {n:G.nodes[n]['eigenvector'] for n in comp_id}\n",
    "\n",
    "    # Then sort that dictionary and print the first n results\n",
    "    print_count=10\n",
    "    comp_sorted_by_eigenvector = sorted(comp_eigenvector.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "    print(\"\\n<--- Modularity Class {} Sorted by Eigenvector Centrality, top {} values --->\".format(n, print_count))\n",
    "    for node in comp_sorted_by_eigenvector[:print_count]:\n",
    "        \n",
    "        degree =  G.nodes[node[0]]['degree']\n",
    "        opinion =  G.nodes[node[0]]['opinions']\n",
    "        \n",
    "        print(\"Name: {}, Eigenvector Centrality: {}, degree: {}, opinion: {}\".format(node[0], \n",
    "                                                                                     node[1],\n",
    "                                                                                     degree,\n",
    "                                                                                     opinion))\n",
    "\n",
    "    largest_comp = components[n]\n",
    "    LCC = G.subgraph(largest_comp)  \n",
    "    \n",
    "    display(\"Number of nodes in current subcomponent: {:,}\".format(LCC.number_of_nodes()))\n",
    "        \n",
    "    # now set colors\n",
    "    opinions = set(nx.get_node_attributes(LCC,'opinions').values())\n",
    "    mapping = dict(zip(sorted(opinions),count()))\n",
    "    \n",
    "    nodes = LCC.nodes()\n",
    "    colors = [mapping[LCC.nodes[n]['opinions']] for n in nodes]\n",
    "    \n",
    "    colors_df = pd.DataFrame(colors, columns=['color'])\n",
    "    colors_agg_df = colors_df.groupby([\"color\"]).apply(lambda x: x['color'].count()).reset_index()\n",
    "    colors_agg_df.rename(columns = {0:'count'}, inplace = True)\n",
    "    \n",
    "    # need to swap keys and values - https://www.geeksforgeeks.org/python-program-to-swap-keys-and-values-in-dictionary/\n",
    "    new_mapping = dict([(value, key) for key, value in mapping.items()])\n",
    "       \n",
    "    colors_agg_df['label'] = ''\n",
    "    colors_agg_df['label'] = colors_agg_df.apply(lambda x: color_lookup(x['color'], new_mapping), axis=1)\n",
    "    \n",
    "    display(colors_agg_df[['label', 'count']].sort_values(by='count', ascending=False))\n",
    "        \n",
    "    if n > 0:\n",
    "        \n",
    "        pos = nx.spring_layout(LCC)\n",
    "    \n",
    "        plt.figure(figsize=(15,15))\n",
    "        plt.axis('off')\n",
    "        nx.draw_networkx(LCC, pos, nodelist=nodes, node_color=colors, \n",
    "                            with_labels=False, node_size=300, cmap=plt.cm.jet)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_range = range(0, 4) # n.b. 0 is too large so takes too long to display\n",
    "\n",
    "for n in my_range:\n",
    "    display_weak_components(n, components, G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "The only component of interest apart from the giant component is component 1 and that is because 140 of 165 nodes have been given a label = 'AGAINST'. However, even in this component, the most important nodes all have label = 'NOT AGAINST' apart from one node, but reviewing Twitter reveals that node to tweet content that is unrelated to crime. Reviewing other nodes within this component reveals the component is actually a discussion on K Pop and so we will not analyse this node any further.\n",
    "\n",
    "This leaves component 0, which we will now investigate in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_zero_nodes = [i for i in G.nodes() if G.nodes[i]['weak_component_id'] == 0]\n",
    "\n",
    "comp_zero_eigenvector = {n:G.nodes[n]['eigenvector'] for n in comp_zero_nodes}\n",
    "\n",
    "comp_sorted_by_eigenvector = sorted(comp_zero_eigenvector.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "print(\"\\n<--- Modularity Class 0 Sorted by Eigenvector Centrality, top 25 values --->\")\n",
    "for node in comp_sorted_by_eigenvector[:25]:\n",
    "        \n",
    "    in_degree =  G.nodes[node[0]]['in_degree']\n",
    "    out_degree =  G.nodes[node[0]]['out_degree']\n",
    "    opinion =  G.nodes[node[0]]['opinions']\n",
    "        \n",
    "    print(\"Name: {}, Eigenvector Centrality: {}, in_degree: {}, out_degree: {}, opinion: {}\".format(node[0], \n",
    "                                                                                 node[1],\n",
    "                                                                                 in_degree,\n",
    "                                                                                 out_degree,\n",
    "                                                                                 opinion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "Reviewing the top 25 tweeters by eigenvector is not particularly useful because it returns a mix of tweeters that are talking about both traffic and crime. However, from this snapshot we can see that nodes with the higher in_degree value relate to nodes that have been critical about the mayor on crime and this merits further investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_zero_nodes = [i for i in G.nodes() if G.nodes[i]['weak_component_id'] == 0]\n",
    "\n",
    "comp_zero_in_degree = {n:G.nodes[n]['in_degree'] for n in comp_zero_nodes}\n",
    "\n",
    "comp_zero_in_degree = sorted(comp_zero_in_degree.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "print(\"\\n<--- Modularity Class 0 Sorted by in degree, top 25 values --->\")\n",
    "for node in comp_zero_in_degree[:25]:\n",
    "        \n",
    "    opinion =  G.nodes[node[0]]['opinions']        \n",
    "    print(\"Name: {}, in_degree: {}, opinion: {}\".format(node[0], \n",
    "                                                                        node[1],\n",
    "                                                                        opinion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "We would expect to see the nodes with the highest in_degree values because this component contains most of the nodes and so in reality not sure this tells us very much about the community beyond it containing most of the nodes. However, reviewing the opinions assigned to the tweeters in this list appears to <b>confirm what we saw previously with the strong networks, which is that the opinion classifier is not a strong predictor of opinions</b>. This is because the majority of tweeters are classified as being NOT_AGAINST the Mayor yet reviewing their tweets on Twitter suggests they aren't fans. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Community detection\n",
    "\n",
    "### 5.1. Identify communities\n",
    "Using networkX, use community detection to identify communities within our network. Also assign community as a node attribute\n",
    "- networkX use greedy modularity to detect communities and so we first need to convert our directed graph to an undirected graph\n",
    "- This code draws heavily on code presented in a tutorial by Ladd [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "H = nx.Graph(G) \n",
    "\n",
    "communities = community.greedy_modularity_communities(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modularity_dict = {} # Create a blank dictionary\n",
    "for i,c in enumerate(communities): # Loop through the list of communities, keeping track of the number for the community\n",
    "    for name in c: # Loop through each person in a community\n",
    "        modularity_dict[name] = i # Create an entry in the dictionary for the person, where the value is which group they belong to.\n",
    "\n",
    "# Now you can add modularity information like we did the other metrics\n",
    "nx.set_node_attributes(G, modularity_dict, 'modularity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_community_dict = pd.DataFrame(list(modularity_dict.items()),columns = ['screen_name','class_id']) \n",
    "class_list = df_community_dict.class_id.unique()\n",
    "\n",
    "print(\"Number of different communities = {} \".format(len(class_list)))\n",
    "\n",
    "df_community_dict_agg = df_community_dict.groupby('class_id').count()\n",
    "\n",
    "comm_count = 10\n",
    "print(\"\\nTop {} communities, by number of members\".format(comm_count))\n",
    "df_community_dict_agg.head(comm_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Analyse key attributes for biggest communities\n",
    "THere are 1,669  different communities and  but the vast majority are immaterial and we can see that the top 10 communities contain nearly 60% of all nodes (15,801 out of a total of 26,691 nodes).\n",
    "\n",
    "We will now analyse the top 10 communities to understand how opinions are expressed across these nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_attributes = dict(G.nodes())\n",
    "node_attribute_values = node_attributes.values()\n",
    "\n",
    "node_attributes_df = pd.DataFrame(node_attribute_values,\n",
    "                      columns =['name', \n",
    "                                'degree',\n",
    "                                'betweenness',\n",
    "                                'eigenvector',\n",
    "                                'opinions',\n",
    "                                'modularity'])\n",
    "\n",
    "node_attributes_df.rename(columns = {'modularity':'community_id'}, inplace = True)\n",
    "node_attributes_df.rename(columns = {'name':'screen_name'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "grouped_community = node_attributes_df.groupby(['community_id', 'opinions']).agg({'screen_name': ['count']})\n",
    "grouped_community.columns = ['count']\n",
    "grouped_community = grouped_community.reset_index()\n",
    "grouped_community = grouped_community[grouped_community.community_id.isin(community_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = px.treemap(grouped_community, path=['community_id', 'opinions'], values='count', color='opinions',\n",
    "                 color_discrete_map={'AGAINST':'rgb(104,132,236)', 'NOT_AGAINST':'rgb(209,101,92)', \n",
    "                                      'FOR':'yellow', \n",
    "                                      'None':'lightgrey'},\n",
    "                title=\"Top communities: proportions AGAINST, NOT_AGAINST, FOR, NONE\")\n",
    "\n",
    "# this is what I don't like, accessing traces like this\n",
    "fig.data[0].textinfo = 'label+text+value+percent parent+percent root'\n",
    "\n",
    "fig.layout.hovermode = False\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(grouped_community, path=['opinions', 'community_id'], values='count', color='opinions',\n",
    "                 color_discrete_map={'AGAINST':'rgb(104,132,236)', 'NOT_AGAINST':'rgb(209,101,92)', \n",
    "                                      'FOR':'yellow', \n",
    "                                      'None':'lightgrey'},\n",
    "                title=\"AGAINST, NOT_AGAINST, FOR, NONE across communities\")\n",
    "\n",
    "# this is what I don't like, accessing traces like this\n",
    "fig.data[0].textinfo = 'label+text+value+percent parent+percent root'\n",
    "\n",
    "fig.layout.hovermode = False\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "The communities in which AGAINST forms the biggest proportion are 0, 5 and 9 and these 3 communities contain 65% of all AGAINST nodes. Of these communities, community 0 presents the most interesting case:\n",
    "- it's the biggest node by size, containing approximately one third of all nodes in the top 10 communities \n",
    "- 54% of nodes in community 0 have opinion = AGAINST, which outperforms the 35% of nodes that are AGAINST across the entire tweeter community\n",
    "- 50% of all nodes with opinion = AGAINST are in community 0\n",
    "\n",
    "We will therefor analyse communities 0, 5 and 9 further and pay particular attention to community 0\n",
    "\n",
    "### 5.3. Visualise and discuss the top communities\n",
    "We will draw the subgraphs for the top 3 communites and color the nodes according to their designated opinion. We will also display the tweeters in the components with the highest eignenvector centrality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "    \n",
    "def print_class_members(class_num, print_count):\n",
    "\n",
    "    # First get a list of just the nodes in that class\n",
    "    class_id = [n for n in G.nodes() if G.nodes[n]['modularity'] == class_num]\n",
    "\n",
    "    # Then create a dictionary of the eigenvector centralities of those nodes\n",
    "    class_eigenvector = {n:G.nodes[n]['eigenvector'] for n in class_id}\n",
    "\n",
    "    # Then sort that dictionary and print the first 5 results\n",
    "    class_sorted_by_eigenvector = sorted(class_eigenvector.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "    print(\"\\n<--- Modularity Class {} Sorted by Eigenvector Centrality, top {} values --->\".format(class_num, count))\n",
    "    for node in class_sorted_by_eigenvector[:print_count]:\n",
    "        \n",
    "        degree =  G.nodes[node[0]]['degree']\n",
    "        opinion =  G.nodes[node[0]]['opinions']\n",
    "        \n",
    "        print(\"Name: {}, Eigenvector Centrality: {}, degree: {}, opinion: {}\".format(node[0], \n",
    "                                                                                     node[1],\n",
    "                                                                                     degree,\n",
    "                                                                                     opinion))\n",
    "        \n",
    "    \n",
    "    my_subgraph = G.subgraph(class_id)\n",
    "    \n",
    "    # now set colors\n",
    "    nodes = my_subgraph.nodes()\n",
    "    opinions = set(nx.get_node_attributes(my_subgraph,'opinions').values())\n",
    "    mapping = dict(zip(sorted(opinions),count()))\n",
    "    \n",
    "    colors = [mapping[my_subgraph.nodes[n]['opinions']] for n in nodes]\n",
    "    \n",
    "    colors_df = pd.DataFrame(colors, columns=['color'])\n",
    "    colors_agg_df = colors_df.groupby([\"color\"]).apply(lambda x: x['color'].count()).reset_index()\n",
    "    colors_agg_df.rename(columns = {0:'count'}, inplace = True)\n",
    "    \n",
    "    # need to swap keys and values - https://www.geeksforgeeks.org/python-program-to-swap-keys-and-values-in-dictionary/\n",
    "    new_mapping = dict([(value, key) for key, value in mapping.items()])\n",
    "       \n",
    "    colors_agg_df['label'] = ''\n",
    "    colors_agg_df['label'] = colors_agg_df.apply(lambda x: color_lookup(x['color'], new_mapping), axis=1)\n",
    "    \n",
    "    display(colors_agg_df[['label', 'count']].sort_values(by='count', ascending=False))\n",
    "    \n",
    "    pos = nx.spring_layout(my_subgraph)\n",
    "    \n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.axis('off')\n",
    "    nx.draw_networkx(my_subgraph, pos, nodelist=nodes, node_color=colors, \n",
    "                            with_labels=False, node_size=300, cmap=plt.cm.jet)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top_member_count = 20\n",
    "\n",
    "community_list = [0, 5, 9]\n",
    "\n",
    "for community_id in community_list:\n",
    "    print(\"<----- class {} ----->\".format(community_id))\n",
    "    print_class_members(community_id, top_member_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "- Community 0\n",
    "    - Our biggest component by some way and the one containing most AGAINST opinions\n",
    "    - Reviewing the most important tweeters having AGAINST label confirmed that label was reasonable for all but 2 cases\n",
    "    - Reviewing the label of a sample of NOT_AGAINST labels suggests a number should have been labeled AGAINST.\n",
    "    - The eignenvectors for a number of these tweeters are high as is their degree which means they are both influential and also prolific retweeters (degree is heavily influenced by 'in' degree).\n",
    "\n",
    "- Community 5\n",
    "    - Definitely speaking about crime and the proportion labeled as NONE means lots of members have been retweeted rather than originated tweets themselves.\n",
    "    - Most of the AGAINST and NOT AGAINST seem reasonably labeled although there were erroneous labels in both classes. Though not many\n",
    "    - This does appear to be a community that is largely AGAINST and although its most important member GBNEWS was labeled NOT_AGAINST, it does host a number of commentators known to be against the Mayor on a range of subjects (masks, traffic and crime).\n",
    "    \n",
    "- Community 9\n",
    "    - Talking about crime but many of the most important nodes have been incorrectly labeled as AGAINST when reviewing their Twitter output suggests otherwise. Interesting that so many in this group but tweets they sent often contained single words like 'stabbed' or 'stabbig' and that led to them being automatically labeled as AGAINST.\n",
    "    - I would conclude that this group is talking about violent crime but many are being mislabeled based on few tweets containing one or two trigger words.\n",
    "\n",
    "### 5.4 Compare membership of strong components and communities\n",
    "we will now see which communities the members of the giant strong component reside in. We are looking to see if there is a relationship. We don't do this for the giant weak component because we know there will be a large overlap by virtue of this component containing almost 80% of all nodes and so it won't really tell us anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_strong_components = pd.DataFrame(components_strong[0], columns=['screen_name'])\n",
    "df_strong_components['component_name'] = 'WCGC'\n",
    "\n",
    "df_community_strong_compare = pd.merge(df_strong_components, node_attributes_df, how='inner', \n",
    "                                       left_on='screen_name', right_on='screen_name')\n",
    "\n",
    "df_community_strong_compare.sort_values(by=['community_id', 'opinions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "31 out of 37 of the WCGC component nodes are within community = 0.\n",
    "Even though this community has a higher proportion of opinions = AGAINST, the majority of key twitter users have been labeled as being not against. Manually reviewing* these users on Twitter suggests the following:\n",
    "- only 1 of the users labeled AGAINST should be labeled NOT AGAINST\n",
    "- 9 of the first 10 users labeled NOT_AGAINST should be labeled against.\n",
    "\n",
    "\n",
    "* The manual review entailed looking up the user screen name in our tweet corpus to see their body of tweets to see if we could assess their opinion. If we could not we looked them up on Twitter, reviewing their bio and between 5-10 tweets. If adverse opinion wasn't observed in the first 10 tweets then an opinion of NOT_AGAINST was assumed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusions\n",
    "The intention of this research and this notebook in particular was to answer the following two research questions:\n",
    "- RQ1: Can Twitter sentiment analysis determine the proportions of Twitter users that accept or reject the London Mayor’s evidence that ‘deprivation in the leading cause of youth violent crime in London’?\n",
    "- RQ2: Can this approach additionally identify the social groups to which users rejecting the Mayor’s evidence belong, and whether this view is widely spread within those groups?\n",
    "\n",
    "### RQ1: Can Twitter sentiment analysis determine the proportions of Twitter users that accept or reject the London Mayor’s evidence that ‘deprivation in the leading cause of youth violent crime in London’?\n",
    "In answer to RQ1, we can conclude say that this approach does not produce a classifier that is accurate enough to predict whether a Twitter user accepts or rejects the London Mayor’s evidence that ‘deprivation in the leading cause of youth violent crime in London’. The evidence we present for this conclusion is three fold:\n",
    "- the classifier trained using hashtags has a recall of 0.57 and a precision of 0.72, which is only moderate performance for a classifier given it will only correctly predict the AGAINST case 57 times out of 100.\n",
    "- the predictions produced by the classifier when applied to a random sample of tweets did not compare well to those produced manually. We found that the classifier both over predicted the AGAINST case, which degrades its precision while simultaneously predicting the wrong label for tweets that SHOULD have been given a label = AGAINST.\n",
    "- investigating membership of strong components, weak components and communities we identified a large proportion of important tweeters who were incorrectly classified as NOT_AGAINST, while reviewing their output on Twitter suggested they should have been labeled as being AGAINST the Mayor. We observed this in all three community variants.\n",
    "\n",
    "We believe the reasons for this is that our tweet corpus contained too few hashtags to be able to train an accurate model, particularly given that we had a vanishingly small number of hashtags supporting the Mayor's argument. In addition, the situation was exacerbated because only 25% of our tweets were original tweets and the remainder were retweets. We provide numbers below:\n",
    "- Only 26% of tweets were original tweets (total number of tweets was 41,706 of which 30,539 were retweets)\n",
    "- NO HASHTAG: 10,294 (92.2%)\n",
    "- AGAINST: 410 (3.7%)\n",
    "- FOR: 12 (0.1%)\n",
    "- Other Hashtag: 451 (4.0%)\n",
    "\n",
    "This resulted in us having a training dataset of just 600 rows and a test dataset of 200 rows. In addition, because we had so few FOR tags, we had to pad the dataset with a random sample of tweets containing other hashtags and this meant the FOR and AGAINST populations contained tweets with features that were not significantly differentiated. We can see this in the word cloud and N Gram plots we produced. This led to a classifier being trained on features based on a small number of words and where those words often occured in both FOR and AGAINST camps. This means there wasn't sufficient specificity in the tweet features to train a strong, generalisable model and this is why the classifier performs so poorly.\n",
    "\n",
    "However, while we failed to produce a classifier that accurately predicts opinions, we do confirm Bovet et al's hypothesis that the success of their approach is dependent on both a reasonable number of hashtags and a good seperation between them. We had neither. Furthermore, applying this approach to an issue rather to an election campaign has in our case been found to be problematic because, while a campaigns offers two or more clear choices, an issue might only raise material interest on one side of the argument. In our case this was mainly AGAINST the Mayor as evidenced by the vanishingly small number of hashtags being explicity for him.\n",
    "\n",
    "### RQ2: Can this approach additionally identify the social groups to which users rejecting the Mayor’s evidence belong, and whether this view is widely spread within those groups?\n",
    "THis is more involved and unfortunately was impacted by the low quality of opinion classifier. This is because the process of identifying opinion within communities would have been more systematic if we could have had more faith in our classifier as we would then have had more trustworthy visual and statistical cues to tell us the consensus opinions in the different components and communities. We still would have needed to double check the opinions of tweets but we could have used our current corpus in more automated fashion to double check that the opinions of a sample of tweeters were as presented in our data. Having to go to Twitter to manually analyse the tweeter output to assess the validity of their opinion label is time consuming, subjective and also not easily repeatable by other researchers as we made a conscious choice to not record the tweets we reviewed as we wanted to preserve their privacy. Having a systematic approach would have allowed us to produce statistics to confirm accuracy instead and hence maintain anonymity while supporting repeatability.\n",
    "\n",
    "So, because the opinion classifier is quite poor, and because this meant we had to manually assess opinions in a number of cases, we have to conclude that this approach is not good for identifying the opinion in the social groups and therefore is cannot easily be used to see how widely spread the opinion is within the different groups.\n",
    "\n",
    "HOWEVER, the strong component and community detection analysis are both very useful exercises and helped expose interesting relationships within our tweeter network. \n",
    "\n",
    "The WCGC contains a large number of important tweeters who interact with each other on a two way basis. Manually investigating the opinion labels given to the most important tweeters showed they were actually against the Mayor and, by virtue of many of them having high in-degree measures, we could conclude they they were prolific retweeters. \n",
    "\n",
    "Many of these important members were also very important members of community = 0, which was our largest community, containing nearly 20% of all tweeters. In fact, 31 out of 37 of the tweeters in the WCGC were also members of community 0. This community is important because it is our largest community and also because the majority of its members were given a predicted label of AGAINST. As we've discussed, we are suspicious of these predictions, but manually checking the opinions of its most important members (by eigenvector) we identified the majority of them as having an AGAINST opinion, even though the predictor has falsely given them a different opinion. High eigenvector values mean a node is influential because it is closely linked to other important members, which means these nodes can disseminate information quickly within the network. Many of these tweeters also had releatively high betweenness centrality, which means these nodes act as brokers between different communities within the network which is also good for nodes wanting to disseminate information quickly. So our community analysis has helped us identify tweeters who talk to each other, retweet prolifically and are able to disseminate information quickly within communities and across to other communities. It would be very useful to investigate these nodes in future research to model diffusion of opinion from these nodes across the network over time.  \n",
    "\n",
    "Finally, the WCGC was less useful because it contains such a high proportion of nodes and so it is difficult to differentiate opinion within this component. However, Bovet looked at this component primarily to see how it was composed over time and how its membership and the relative opinions of that membership changed in response to significant events, such as debates and conferences. We were not able to assess this within our research because the Twitter API limited us to extracting the most recent tweets, and so we only had Twitter actvity for one month, and this was during the summer months which is normally a quieter period for politics. Our original intention was to extract tweets over a far longer period and to see how membership changed in response to the Mayor issuing evidence on the causes of crime and also the campaigns for the Mayoral elections in which crime was a key issue. However, this was not possible and so limits the benefits of reviewing the WCGC.\n",
    "\n",
    "### Ideas for further research\n",
    "We have already discussed looking at information diffusion to understand how opinions spread in the network but it would also be interesting to invert the label propogation approach to see if this can create a more sizeable corpus with which to train our model. \n",
    "- Use community detection and strong component analysis to identify important tweeters and then label them using the tweet corpus or Twitter itself\n",
    "- Use label propogation through the tweeter network rather than the hashtag network in order to set opinions on a tweeter basis. \n",
    "- Then cascade the tweeter opinion to their tweets and see if this creates a large enough body of tweets on which to train a classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A - Glossary\n",
    "- Directed Graphs - a network (graph) comprises nodes (vertices) and links between these nodes (edges). In an undirected graph we purely capture whether a link exists between two nodes, not it's direction, namely we do not know if this link is from A to B, B to A or traverses both ways. In a directed graph, we do capture this information and this is important if we want to analyse strong and weak components within the network\n",
    "- Strong Component: subgraphs within a network which comprise a set of connected nodes where every edge traverses in both directions. This two way communication represents a more tightly coupled community because the communication is bi-directional. It is worthwhile briefly discussing the method used to derive the strong components and the specific implementation we will employ.\n",
    "    - Starting at any node, traverse all edges from that node and repeat for all connected nodes until we cannot reach any further node. This is a search or a path. Start at a new node and repeat until there are no further unexplored nodes. These traversals produce a set of paths.\n",
    "    - Find the root for each path and then remove any connected nodes that are also members of other paths. Having done this we are left with our strongly connected components.\n",
    "    - The size of these components represents their importance and the largest of these components is the Strong Component Giant Component, which we discussed previously.\n",
    "    - We will use the networkX implementation to derive the SCGC and this implements an algorithm by Nuutila and Soisalon-Soinen [4] which in turn optimises the original algorithm by R. Tarjan [2]. Tarjan's algorithm performs as discussed previously but Nuutila improves the performance of the second traversal by optimising which nodes are considered.\n",
    "- Weak Component: subgraphs within a network where communication between nodes is one way only. In essence, if we ignore the direction of an edge then a strong component is also a weak component (but not vice versa) and therefore similar, though simpler logic, can be used to identify them. In addition, because their size is not limited to edges that are two way, we can expect weak components to be larger than strong components, namely we expect them to comprise a greater number of vertices.\n",
    "- Degree - this is the number of edges in and out of a node. A node with a higher number of edges will have a higher degree and this is a measure of how connected this node is. Ladd et al [5] describe highly connected nodes as hubs and this is a useful analogy.\n",
    "- Eigenvector - this is an extention of the degree but considers the importance of the nodes each node is connected to. For example, if a node is a hub and it is immediately connected to other hubs then it will be given a higher eigenvector score because this suggests this node is important. This importance arises because it is a measure of how quickly this node can disseminate information through the network. For example, a hub node connected to multiple other hub nodes can disseminate information more quickly and is therefore more important. Eigenvectors are very closely related to the page rank measure, the basis for Google's search engine.\n",
    "- Betweenness - As discussed by Ladd [5] betweeness centrality is a measure of how well a node connects different parts of the network, and describe which nodes which provide the shortest routes between different parts of the network. Ladd describes them as broker nodes and this is a good analogy because these nodes exchange information between network segments and so facilitate quicker information dissemination. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix B - References\n",
    "- [1] Bovet, A., Morone, F. and Makse, H.A., 2018. Validation of Twitter opinion trends with national polling aggregates: Hillary Clinton vs Donald Trump. Scientific reports, 8(1), pp.1-16.\n",
    "- [2] https://networkx.org/documentation/networkx-1.9/reference/generated/networkx.algorithms.components.strongly_connected.strongly_connected_components.html\n",
    "- [3] R. Tarjan SIAM Journal of Computing 1(2):146-160, (1972). Depth-first search and linear graph algorithms \n",
    "- [4] E. Nuutila and E. Soisalon-Soinen Information Processing Letters 49(1): 9-14, (1994). On finding the strongly connected components in a directed graph.\n",
    "- [5] John R. Ladd, Jessica Otis, Christopher N. Warren, and Scott Weingart, \"Exploring and Analyzing Network Data with Python,\" The Programming Historian 6 (2017), https://doi.org/10.46430/phen0064.\n",
    "- [6] https://github.com/alexbovet/network_lesson/blob/master/02_Analysis_of_Twitter_Social_Network.ipynb\n",
    "- [7]  https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.algorithms.components.connected.connected_component_subgraphs.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
