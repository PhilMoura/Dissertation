{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social networks\n",
    "I basically use two sources for this, Bovet and programminghistorian\n",
    "\n",
    "What I want to find out is basically the following:\n",
    "1. Are there obvious social clusters within our network\n",
    "2. If so, how big are they and how strong are the relationships\n",
    "3. Within these networks, who are the most important tweeters\n",
    "\n",
    "We also want to know the following general information:\n",
    "- Which users are most important, irrespective of network\n",
    "\n",
    "## Changes I need to make\n",
    "- Bring strong connections up front - say not really informative but keep an eye out for key players\n",
    "- Then do overview of degrees and centrality - explain why\n",
    "- Get components + communities\n",
    "- Do page ranking - explain why and compare with degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24772, 25)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>tweeter_id</th>\n",
       "      <th>tweeter_user_name</th>\n",
       "      <th>tweeter_screen_name</th>\n",
       "      <th>tweeter_location</th>\n",
       "      <th>message_text</th>\n",
       "      <th>in_reply_to_user_screen_name</th>\n",
       "      <th>quote_tweet_screen_name</th>\n",
       "      <th>...</th>\n",
       "      <th>mentioned</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>Tweet_punct</th>\n",
       "      <th>Tweet_tokenized</th>\n",
       "      <th>Tweet_nonstop</th>\n",
       "      <th>Tweet_lemmatized</th>\n",
       "      <th>Clean_MessageText</th>\n",
       "      <th>tweet_date_dt</th>\n",
       "      <th>time_bins_12h</th>\n",
       "      <th>day_bins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1420355029081395203</td>\n",
       "      <td>2021-07-28 12:06:42</td>\n",
       "      <td>865262041103302656</td>\n",
       "      <td>Chris Myers</td>\n",
       "      <td>myerschrismyer1</td>\n",
       "      <td>Richmond North Yorkshire</td>\n",
       "      <td>RT @LeslieH24367191: 'Do better!' Boris Johnso...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Do better Boris Johnson issues scathing respo...</td>\n",
       "      <td>['do', 'better', 'boris', 'johnson', 'issues',...</td>\n",
       "      <td>['better', 'boris', 'johnson', 'issues', 'scat...</td>\n",
       "      <td>['good', 'boris', 'johnson', 'issues', 'scathi...</td>\n",
       "      <td>good boris johnson issues scathing response sa...</td>\n",
       "      <td>2021-07-28 12:06:42</td>\n",
       "      <td>2021-07-28 12:00:00</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1420354120041615364</td>\n",
       "      <td>2021-07-28 12:03:05</td>\n",
       "      <td>229645453</td>\n",
       "      <td>Emily Sheffield</td>\n",
       "      <td>emilysheffield</td>\n",
       "      <td>London</td>\n",
       "      <td>RT @NaheedMajeed: Spot on @standardnews With L...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['@standardnews']</td>\n",
       "      <td>[]</td>\n",
       "      <td>Spot on  With London on track to break record...</td>\n",
       "      <td>['spot', 'on', 'with', 'london', 'on', 'track'...</td>\n",
       "      <td>['spot', 'track', 'break', 'record', 'teenage'...</td>\n",
       "      <td>['spot', 'track', 'break', 'record', 'teenage'...</td>\n",
       "      <td>spot track break record teenage murdersstop se...</td>\n",
       "      <td>2021-07-28 12:03:05</td>\n",
       "      <td>2021-07-28 12:00:00</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1420351785156366337</td>\n",
       "      <td>2021-07-28 11:53:49</td>\n",
       "      <td>2843896642</td>\n",
       "      <td>Basilewitch</td>\n",
       "      <td>basilewitch</td>\n",
       "      <td>France</td>\n",
       "      <td>RT @Short2Cjs: Boris today speaking to Nick Fe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Boris today speaking to Nick Ferrari about Kh...</td>\n",
       "      <td>['boris', 'today', 'speaking', 'to', 'nick', '...</td>\n",
       "      <td>['boris', 'today', 'speaking', 'nick', 'ferrar...</td>\n",
       "      <td>['boris', 'today', 'speaking', 'nick', 'ferrar...</td>\n",
       "      <td>boris today speaking nick ferrari khan … done ...</td>\n",
       "      <td>2021-07-28 11:53:49</td>\n",
       "      <td>2021-07-28 00:00:00</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1420350419449094148</td>\n",
       "      <td>2021-07-28 11:48:23</td>\n",
       "      <td>556315551</td>\n",
       "      <td>Dean Cowcher</td>\n",
       "      <td>DeanCowcher</td>\n",
       "      <td>Bexleyheath London England</td>\n",
       "      <td>RT @LeslieH24367191: 'Do better!' Boris Johnso...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Do better Boris Johnson issues scathing respo...</td>\n",
       "      <td>['do', 'better', 'boris', 'johnson', 'issues',...</td>\n",
       "      <td>['better', 'boris', 'johnson', 'issues', 'scat...</td>\n",
       "      <td>['good', 'boris', 'johnson', 'issues', 'scathi...</td>\n",
       "      <td>good boris johnson issues scathing response sa...</td>\n",
       "      <td>2021-07-28 11:48:23</td>\n",
       "      <td>2021-07-28 00:00:00</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1420349615954669574</td>\n",
       "      <td>2021-07-28 11:45:11</td>\n",
       "      <td>1267062754915090434</td>\n",
       "      <td>Alfiecat</td>\n",
       "      <td>sonya_annie</td>\n",
       "      <td>unknown</td>\n",
       "      <td>RT @Short2Cjs: Boris today speaking to Nick Fe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Boris today speaking to Nick Ferrari about Kh...</td>\n",
       "      <td>['boris', 'today', 'speaking', 'to', 'nick', '...</td>\n",
       "      <td>['boris', 'today', 'speaking', 'nick', 'ferrar...</td>\n",
       "      <td>['boris', 'today', 'speaking', 'nick', 'ferrar...</td>\n",
       "      <td>boris today speaking nick ferrari khan … done ...</td>\n",
       "      <td>2021-07-28 11:45:11</td>\n",
       "      <td>2021-07-28 00:00:00</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index             tweet_id           tweet_date           tweeter_id  \\\n",
       "0      0  1420355029081395203  2021-07-28 12:06:42   865262041103302656   \n",
       "1      1  1420354120041615364  2021-07-28 12:03:05            229645453   \n",
       "2      2  1420351785156366337  2021-07-28 11:53:49           2843896642   \n",
       "3      3  1420350419449094148  2021-07-28 11:48:23            556315551   \n",
       "4      4  1420349615954669574  2021-07-28 11:45:11  1267062754915090434   \n",
       "\n",
       "  tweeter_user_name tweeter_screen_name            tweeter_location  \\\n",
       "0       Chris Myers     myerschrismyer1    Richmond North Yorkshire   \n",
       "1   Emily Sheffield      emilysheffield                      London   \n",
       "2       Basilewitch         basilewitch                      France   \n",
       "3      Dean Cowcher         DeanCowcher  Bexleyheath London England   \n",
       "4          Alfiecat         sonya_annie                     unknown   \n",
       "\n",
       "                                        message_text  \\\n",
       "0  RT @LeslieH24367191: 'Do better!' Boris Johnso...   \n",
       "1  RT @NaheedMajeed: Spot on @standardnews With L...   \n",
       "2  RT @Short2Cjs: Boris today speaking to Nick Fe...   \n",
       "3  RT @LeslieH24367191: 'Do better!' Boris Johnso...   \n",
       "4  RT @Short2Cjs: Boris today speaking to Nick Fe...   \n",
       "\n",
       "  in_reply_to_user_screen_name quote_tweet_screen_name  ...  \\\n",
       "0                          NaN                     NaN  ...   \n",
       "1                          NaN                     NaN  ...   \n",
       "2                          NaN                     NaN  ...   \n",
       "3                          NaN                     NaN  ...   \n",
       "4                          NaN                     NaN  ...   \n",
       "\n",
       "           mentioned  hashtags  \\\n",
       "0                 []        []   \n",
       "1  ['@standardnews']        []   \n",
       "2                 []        []   \n",
       "3                 []        []   \n",
       "4                 []        []   \n",
       "\n",
       "                                         Tweet_punct  \\\n",
       "0   Do better Boris Johnson issues scathing respo...   \n",
       "1   Spot on  With London on track to break record...   \n",
       "2   Boris today speaking to Nick Ferrari about Kh...   \n",
       "3   Do better Boris Johnson issues scathing respo...   \n",
       "4   Boris today speaking to Nick Ferrari about Kh...   \n",
       "\n",
       "                                     Tweet_tokenized  \\\n",
       "0  ['do', 'better', 'boris', 'johnson', 'issues',...   \n",
       "1  ['spot', 'on', 'with', 'london', 'on', 'track'...   \n",
       "2  ['boris', 'today', 'speaking', 'to', 'nick', '...   \n",
       "3  ['do', 'better', 'boris', 'johnson', 'issues',...   \n",
       "4  ['boris', 'today', 'speaking', 'to', 'nick', '...   \n",
       "\n",
       "                                       Tweet_nonstop  \\\n",
       "0  ['better', 'boris', 'johnson', 'issues', 'scat...   \n",
       "1  ['spot', 'track', 'break', 'record', 'teenage'...   \n",
       "2  ['boris', 'today', 'speaking', 'nick', 'ferrar...   \n",
       "3  ['better', 'boris', 'johnson', 'issues', 'scat...   \n",
       "4  ['boris', 'today', 'speaking', 'nick', 'ferrar...   \n",
       "\n",
       "                                    Tweet_lemmatized  \\\n",
       "0  ['good', 'boris', 'johnson', 'issues', 'scathi...   \n",
       "1  ['spot', 'track', 'break', 'record', 'teenage'...   \n",
       "2  ['boris', 'today', 'speaking', 'nick', 'ferrar...   \n",
       "3  ['good', 'boris', 'johnson', 'issues', 'scathi...   \n",
       "4  ['boris', 'today', 'speaking', 'nick', 'ferrar...   \n",
       "\n",
       "                                   Clean_MessageText        tweet_date_dt  \\\n",
       "0  good boris johnson issues scathing response sa...  2021-07-28 12:06:42   \n",
       "1  spot track break record teenage murdersstop se...  2021-07-28 12:03:05   \n",
       "2  boris today speaking nick ferrari khan … done ...  2021-07-28 11:53:49   \n",
       "3  good boris johnson issues scathing response sa...  2021-07-28 11:48:23   \n",
       "4  boris today speaking nick ferrari khan … done ...  2021-07-28 11:45:11   \n",
       "\n",
       "         time_bins_12h day_bins  \n",
       "0  2021-07-28 12:00:00       28  \n",
       "1  2021-07-28 12:00:00       28  \n",
       "2  2021-07-28 00:00:00       28  \n",
       "3  2021-07-28 00:00:00       28  \n",
       "4  2021-07-28 00:00:00       28  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets = pd.read_csv(\"./DataSources/TwitterData/cleaned_tweets_20210806.csv\")\n",
    "print(all_tweets.shape)\n",
    "all_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before after dropping rows with all NaN\n",
      "(24772, 25)\n",
      "index                           24741\n",
      "tweet_id                        24741\n",
      "tweet_date                      24741\n",
      "tweeter_id                      24741\n",
      "tweeter_user_name               24741\n",
      "tweeter_screen_name             24741\n",
      "tweeter_location                24741\n",
      "message_text                    24741\n",
      "in_reply_to_user_screen_name     2608\n",
      "quote_tweet_screen_name           425\n",
      "favourite_count                 24741\n",
      "retweet_count                   24741\n",
      "extract_run_date                24741\n",
      "retrieved_using_search_term     24741\n",
      "retweeted                       24741\n",
      "mentioned                       24741\n",
      "hashtags                        24741\n",
      "Tweet_punct                     24741\n",
      "Tweet_tokenized                 24741\n",
      "Tweet_nonstop                   24741\n",
      "Tweet_lemmatized                24741\n",
      "Clean_MessageText               24739\n",
      "tweet_date_dt                   24741\n",
      "time_bins_12h                   24741\n",
      "day_bins                        24741\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "all_tweets = all_tweets.dropna(how='all') # only drops a row when every column is NA\n",
    "print(\"shape before after dropping rows with all NaN\")\n",
    "print(all_tweets.shape)\n",
    "\n",
    "# Now check for individual NaN values\n",
    "nan_values = all_tweets[all_tweets.isna().any(axis=1)]\n",
    "print(nan_values.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index                           2\n",
      "tweet_id                        2\n",
      "tweet_date                      2\n",
      "tweeter_id                      2\n",
      "tweeter_user_name               2\n",
      "tweeter_screen_name             2\n",
      "tweeter_location                2\n",
      "message_text                    2\n",
      "in_reply_to_user_screen_name    2\n",
      "quote_tweet_screen_name         2\n",
      "favourite_count                 2\n",
      "retweet_count                   2\n",
      "extract_run_date                2\n",
      "retrieved_using_search_term     2\n",
      "retweeted                       2\n",
      "mentioned                       2\n",
      "hashtags                        2\n",
      "Tweet_punct                     2\n",
      "Tweet_tokenized                 2\n",
      "Tweet_nonstop                   2\n",
      "Tweet_lemmatized                2\n",
      "Clean_MessageText               0\n",
      "tweet_date_dt                   2\n",
      "time_bins_12h                   2\n",
      "day_bins                        2\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>tweeter_id</th>\n",
       "      <th>tweeter_user_name</th>\n",
       "      <th>tweeter_screen_name</th>\n",
       "      <th>tweeter_location</th>\n",
       "      <th>message_text</th>\n",
       "      <th>in_reply_to_user_screen_name</th>\n",
       "      <th>quote_tweet_screen_name</th>\n",
       "      <th>...</th>\n",
       "      <th>mentioned</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>Tweet_punct</th>\n",
       "      <th>Tweet_tokenized</th>\n",
       "      <th>Tweet_nonstop</th>\n",
       "      <th>Tweet_lemmatized</th>\n",
       "      <th>Clean_MessageText</th>\n",
       "      <th>tweet_date_dt</th>\n",
       "      <th>time_bins_12h</th>\n",
       "      <th>day_bins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6085</th>\n",
       "      <td>6085</td>\n",
       "      <td>1417805178540105730</td>\n",
       "      <td>2021-07-21 11:14:30</td>\n",
       "      <td>1397801811935956993</td>\n",
       "      <td>Jimmy</td>\n",
       "      <td>Jimmy20723610</td>\n",
       "      <td>Mother Earth</td>\n",
       "      <td>@BraddockBessie https://t.co/23Cde1u6r3\\nhttps...</td>\n",
       "      <td>BraddockBessie</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>['@BraddockBessie']</td>\n",
       "      <td>[]</td>\n",
       "      <td>\\n\\n\\n</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-07-21 11:14:30</td>\n",
       "      <td>2021-07-21 00:00:00</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17157</th>\n",
       "      <td>17157</td>\n",
       "      <td>1421529556121489410</td>\n",
       "      <td>2021-07-31 17:53:51</td>\n",
       "      <td>279478380</td>\n",
       "      <td>Artur.Kli</td>\n",
       "      <td>ArturK_IE</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>@forwardnotbac @MayorofLondon Sure is https://...</td>\n",
       "      <td>forwardnotbac</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>['@forwardnotbac', '@MayorofLondon']</td>\n",
       "      <td>[]</td>\n",
       "      <td>Sure is</td>\n",
       "      <td>['sure', 'is']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-07-31 17:53:51</td>\n",
       "      <td>2021-07-31 12:00:00</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index             tweet_id           tweet_date           tweeter_id  \\\n",
       "6085    6085  1417805178540105730  2021-07-21 11:14:30  1397801811935956993   \n",
       "17157  17157  1421529556121489410  2021-07-31 17:53:51            279478380   \n",
       "\n",
       "      tweeter_user_name tweeter_screen_name tweeter_location  \\\n",
       "6085              Jimmy       Jimmy20723610     Mother Earth   \n",
       "17157         Artur.Kli           ArturK_IE          Ireland   \n",
       "\n",
       "                                            message_text  \\\n",
       "6085   @BraddockBessie https://t.co/23Cde1u6r3\\nhttps...   \n",
       "17157  @forwardnotbac @MayorofLondon Sure is https://...   \n",
       "\n",
       "      in_reply_to_user_screen_name quote_tweet_screen_name  ...  \\\n",
       "6085                BraddockBessie                          ...   \n",
       "17157                forwardnotbac                          ...   \n",
       "\n",
       "                                  mentioned  hashtags  Tweet_punct  \\\n",
       "6085                    ['@BraddockBessie']        []      \\n\\n\\n    \n",
       "17157  ['@forwardnotbac', '@MayorofLondon']        []     Sure is    \n",
       "\n",
       "      Tweet_tokenized Tweet_nonstop Tweet_lemmatized Clean_MessageText  \\\n",
       "6085               []            []               []               NaN   \n",
       "17157  ['sure', 'is']            []               []               NaN   \n",
       "\n",
       "             tweet_date_dt        time_bins_12h day_bins  \n",
       "6085   2021-07-21 11:14:30  2021-07-21 00:00:00       21  \n",
       "17157  2021-07-31 17:53:51  2021-07-31 12:00:00       31  \n",
       "\n",
       "[2 rows x 25 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to set the in_reply_to_user_screen_name and quote_tweet_screen_name fields to blanks\n",
    "all_tweets.loc[all_tweets['in_reply_to_user_screen_name'].isna(), 'in_reply_to_user_screen_name'] = ''\n",
    "all_tweets.loc[all_tweets['quote_tweet_screen_name'].isna(), 'quote_tweet_screen_name'] = ''\n",
    "\n",
    "# Now check for individual NaN values\n",
    "nan_values = all_tweets[all_tweets.isna().any(axis=1)]\n",
    "print(nan_values.count())\n",
    "\n",
    "nan_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network of interactions\n",
    "### This code draws heavily on the work of Bovet\n",
    "### https://github.com/alexbovet/network_lesson/blob/master/02_Analysis_of_Twitter_Social_Network.ipynb\n",
    "\n",
    "We will use the python module NetworkX to construct and analyze the social network.\n",
    "\n",
    "There are four types of interactions between two users in Twitter:\n",
    "- Retweet\n",
    "- Quote\n",
    "- Reply\n",
    "- Mention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define some functions to extract the interactions from tweets\n",
    "def string_to_list(my_str):\n",
    "    delimiter = \",\"\n",
    "    my_str = my_str.replace(\"[\", \"\")\n",
    "    my_str = my_str.replace(\"]\", \"\")\n",
    "    my_str = my_str.replace(\"@\", \"\")\n",
    "    my_str = my_str.replace(\"'\", \"\")\n",
    "    my_str = my_str.replace(\" \", \"\")\n",
    "    my_list = my_str.split(delimiter)\n",
    "    return my_list\n",
    "\n",
    "def getAllInteractions(tweet):\n",
    "    \n",
    "    # Get the tweeter\n",
    "    tweet_id = tweet.tweet_id\n",
    "    tweeter_id = tweet.tweeter_id\n",
    "    tweeter_name = tweet.tweeter_screen_name\n",
    "    \n",
    "    # a python set is a collection of unique items\n",
    "    # we use a set to avoid duplicated ids\n",
    "    interacting_users = set()\n",
    "    \n",
    "    # Add person they're replying to\n",
    "    if tweet.in_reply_to_user_screen_name != '':\n",
    "        interacting_users.add(tweet.in_reply_to_user_screen_name)\n",
    "        \n",
    "    # Add person they quoted\n",
    "    if tweet.quote_tweet_screen_name != '':\n",
    "        interacting_users.add(tweet.quote_tweet_screen_name)\n",
    "    \n",
    "    # Add person they retweeted\n",
    "    if len(tweet.retweeted) > 2: # because empty strings will contain []\n",
    "        retweeted_list = string_to_list(tweet.retweeted)\n",
    "        for item in retweeted_list:\n",
    "            interacting_users.add(item)\n",
    "       \n",
    "    # Add mentions\n",
    "    if len(tweet.mentioned) > 2: # because empty strings will contain []\n",
    "        mentioned_list = string_to_list(tweet.mentioned)\n",
    "        for item in mentioned_list:\n",
    "            interacting_users.add(item)\n",
    "  \n",
    "    # remove the tweeter if he is in the set\n",
    "    interacting_users.discard(tweeter_name)\n",
    "    \n",
    "    # Return our tweeter and their influencers\n",
    "    return tweeter_id, tweeter_name, tweet_id, list(interacting_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# define an empty Directed Graph\n",
    "# A directed graph is a graph where edges have a direction\n",
    "# in our case the edges goes from user that sent the tweet to\n",
    "# the user with whom they interacted (retweeted, mentioned or quoted)\n",
    "G = nx.DiGraph()\n",
    "\n",
    "for index, tweet in all_tweets.iterrows():\n",
    "    \n",
    "    if (tweet.tweeter_screen_name != 'SadiqKhan') & (tweet.tweeter_screen_name != 'MayorofLondon'):\n",
    "        tweeter_id, tweeter_name, tweet_id, interactions = getAllInteractions(tweet)\n",
    "    \n",
    "        # add an edge to the Graph for each influencer\n",
    "        for interact_name in interactions:\n",
    "        \n",
    "            # add edges between the two user ids\n",
    "            # this will create new nodes if the nodes are not already in the network\n",
    "            # we also add an attribute the to edge equal to the id of the tweet\n",
    "            G.add_edge(tweeter_name, interact_name, tweet_id=tweet_id)\n",
    "        \n",
    "            # add name as a property to each node\n",
    "            # with networkX each node is a dictionary\n",
    "            G.nodes[tweeter_name]['name'] = tweeter_name\n",
    "            G.nodes[interact_name]['name'] = interact_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with network characteristics\n",
    "Based on tutorial from: https://programminghistorian.org/en/lessons/exploring-and-analyzing-network-data-with-python \n",
    "- John R. Ladd, Jessica Otis, Christopher N. Warren, and Scott Weingart, \"Exploring and Analyzing Network Data with Python,\" The Programming Historian 6 (2017), https://doi.org/10.46430/phen0064."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: DiGraph\n",
      "Number of nodes: 17220\n",
      "Number of edges: 25947\n",
      "Average in degree:   1.5068\n",
      "Average out degree:   1.5068\n"
     ]
    }
   ],
   "source": [
    "print(nx.info(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network density: 8.750766159980882e-05\n"
     ]
    }
   ],
   "source": [
    "# On a scale of 0 to 1, where 1 is a dense network\n",
    "density = nx.density(G)\n",
    "print(\"Network density:\", density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_dict = dict(G.degree(G.nodes()))\n",
    "nx.set_node_attributes(G, degree_dict, 'degree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the keys are the user_id\n",
    "nodelist = list(G.nodes.keys())\n",
    "print(nodelist[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each node is itself a dictionary with node attributes as key,value pairs\n",
    "print(type(G.nodes[nodelist[3]]))\n",
    "print(G.nodes[nodelist[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from networkx.algorithms import community \n",
    "\n",
    "sorted_degree = sorted(degree_dict.items(), key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 20 nodes by degree:\")\n",
    "for d in sorted_degree[:20]:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betweenness_dict = nx.betweenness_centrality(G) # Run betweenness centrality\n",
    "eigenvector_dict = nx.eigenvector_centrality(G) # Run eigenvector centrality\n",
    "\n",
    "# Assign each to an attribute in your network\n",
    "nx.set_node_attributes(G, betweenness_dict, 'betweenness')\n",
    "nx.set_node_attributes(G, eigenvector_dict, 'eigenvector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_betweenness = sorted(betweenness_dict.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "print(\"Top 20 nodes by betweenness centrality:\")\n",
    "for b in sorted_betweenness[:20]:\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First get the top 20 nodes by betweenness as a list\n",
    "top_betweenness = sorted_betweenness[:20]\n",
    "\n",
    "#Then find and print their degree\n",
    "for tb in top_betweenness: # Loop through top_betweenness\n",
    "    degree = degree_dict[tb[0]] # Use degree_dict to access a node's degree, see footnote 2\n",
    "    print(\"Name:\", tb[0], \"| Betweenness Centrality:\", tb[1], \"| Degree:\", degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = nx.Graph(G) # need to convert to undirected graph to use greedy_modularity\n",
    "\n",
    "communities = community.greedy_modularity_communities(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modularity_dict = {} # Create a blank dictionary\n",
    "for i,c in enumerate(communities): # Loop through the list of communities, keeping track of the number for the community\n",
    "    for name in c: # Loop through each person in a community\n",
    "        modularity_dict[name] = i # Create an entry in the dictionary for the person, where the value is which group they belong to.\n",
    "\n",
    "# Now you can add modularity information like we did the other metrics\n",
    "nx.set_node_attributes(G, modularity_dict, 'modularity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_community_dict = pd.DataFrame(list(modularity_dict.items()),columns = ['screen_name','class_id']) \n",
    "class_list = df_community_dict.class_id.unique()\n",
    "\n",
    "print(\"Number of different communities = {} \".format(len(class_list)))\n",
    "\n",
    "df_community_dict_agg = df_community_dict.groupby('class_id').count()\n",
    "\n",
    "comm_count = 10\n",
    "print(\"\\nTop {} communities, by number of members\".format(comm_count))\n",
    "df_community_dict_agg.head(comm_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def print_class_members(class_num, count):\n",
    "\n",
    "    # First get a list of just the nodes in that class\n",
    "    class_id = [n for n in G.nodes() if G.nodes[n]['modularity'] == class_num]\n",
    "\n",
    "    # Then create a dictionary of the eigenvector centralities of those nodes\n",
    "    class_eigenvector = {n:G.nodes[n]['eigenvector'] for n in class_id}\n",
    "\n",
    "    # Then sort that dictionary and print the first 5 results\n",
    "    class_sorted_by_eigenvector = sorted(class_eigenvector.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "    print(\"\\n<--- Modularity Class {} Sorted by Eigenvector Centrality, top {} values --->\".format(class_num, count))\n",
    "    for node in class_sorted_by_eigenvector[:count]:\n",
    "        print(\"Name:\", node[0], \"| Eigenvector Centrality:\", node[1])\n",
    "        \n",
    "    my_subgraph = G.subgraph(class_id)\n",
    "    \n",
    "    plt.figure()\n",
    "    nx.draw(my_subgraph)\n",
    "    plt.show()\n",
    "        \n",
    "print_class_members(0, 10)\n",
    "print_class_members(1, 10)\n",
    "print_class_members(2, 10)\n",
    "print_class_members(3, 10)\n",
    "print_class_members(4, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## carry on with bovet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edges are contained in a EdgeView with a set-like interface\n",
    "print(type(G.edges))\n",
    "print(G.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see all the edges going out of this node\n",
    "# each edge is a dictionary inside this dictionary with a key \n",
    "# corresponding to the target user_id\n",
    "e = G.out_edges(nodelist[4], data=True)\n",
    "print(nodelist[4])\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can iterate over the out-edges \n",
    "for s,t,data in e:\n",
    "    print(s,t,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary with the degree of all nodes\n",
    "all_degrees = [G.degree(n) for n in nodelist] # this is the degree for undirected edges\n",
    "in_degrees = [G.in_degree(n) for n in nodelist]\n",
    "out_degrees = [G.out_degree(n) for n in nodelist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average degree\n",
    "2*G.number_of_edges()/G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.array(all_degrees).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(in_degrees).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(out_degrees).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to make a list with (user_id, username, degree) for all nodes\n",
    "degree_node_list = []\n",
    "for node in nodelist:\n",
    "    degree_node_list.append((node, G.nodes[node]['name'], G.degree(node)))\n",
    "    \n",
    "print('Unordered user, degree list')    \n",
    "print(degree_node_list[:10])\n",
    "\n",
    "# sort the list according the degree in descinding order\n",
    "degree_node_list = sorted(degree_node_list, key=lambda x:x[2], reverse=True)\n",
    "print('Ordered user, degree list')    \n",
    "print(degree_node_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to import matplolib for making plots\n",
    "# and numpy for numerical computations\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network components\n",
    "Connected components are a subset of nodes in which each node has a minimum of one link with another node in the same subset, and any node that is a member of 'this' subset is not linked to any nodes external to the subset. \n",
    "\n",
    "In addition, for <b> directed </b> graphs we can define two types of connected components:\n",
    "- Weakly connected components, (WCC): maximal set of nodes where there exists a path in at least one direction between each pair of nodes.\n",
    "- Strongly connected components, (SCC): maximal set of nodes where there exists a path in both directions between each pair of nodes.\n",
    "\n",
    "And, within these categories we can additionally identify the weakly connected giant (WCGC), which is the largest of the weakly connected components and the strongly connected giant (SCGC), which is the largest of the strongly connected components. In effect these are the largest subgraphs within the network. Bovet et al illustrate this (incorrectly, for SCGC given arrows aren't two way) as follows:\n",
    "\n",
    "<img src=\"WCGC-SCGC.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strongly connected components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this returns a list of set of nodes belonging to the \n",
    "# different (strongly) connected components\n",
    "components_strong = list(nx.strongly_connected_components(G))\n",
    "\n",
    "# sort the component according to their size\n",
    "components_strong = list(sorted(components_strong, key=lambda x:len(x), reverse=True))\n",
    "\n",
    "# make a list with the size of each component\n",
    "comp_sizes_strong = []\n",
    "for comp in components_strong:\n",
    "    comp_sizes_strong.append(len(comp))\n",
    "    \n",
    "print(\"number of strong components: {}\".format(len(comp_sizes_strong)))\n",
    "\n",
    "print(\"sizes of the ten largest components: {}\".format(comp_sizes_strong[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_strongly_connected(idx):\n",
    "\n",
    "    largest_comp_strong = components_strong[idx]\n",
    "    LCC_strong = G.subgraph(largest_comp_strong)\n",
    "\n",
    "    print(\"number of components in LCC[{}] = {}\".format(idx, LCC_strong.number_of_nodes()))\n",
    "\n",
    "    # let's plot the degree distribution inside the LCC\n",
    "    degrees = [LCC_strong.degree(n) for n in LCC_strong.nodes()]\n",
    "    print(\"number of degrees for each node within LCC[{}] = {}\\n\".format(idx, degrees))\n",
    "    \n",
    "    # add weights to the edges based on how many times they are traversed\n",
    "    print(\"Weight of edges between each node:\\n\")\n",
    "    \n",
    "    for u, v, d in LCC_strong.edges(data=True):\n",
    "        d['weight'] = 1\n",
    "    for u,v,d in LCC_strong.edges(data=True):\n",
    "        print (u,v,d)\n",
    "    \n",
    "    ## <--- now look at centrality\n",
    "    betweenValues = nx.betweenness_centrality(LCC_strong)\n",
    "    \n",
    "    # betweenValues is a dictionary, let's get the values and keys in separate lists\n",
    "    values = list(betweenValues.values())\n",
    "    keys = list(betweenValues.keys())\n",
    "    \n",
    "    # find the index of the node with highest betweeness centrality\n",
    "    highestIndex = np.argmax(values)\n",
    "    \n",
    "    #Uses solution in the documentation: https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.algorithms.components.connected.connected_component_subgraphs.html\n",
    "    \n",
    "    print('\\n <---- centrality and clustering measures -------> \\n')\n",
    "    print(\"The node id \", keys[highestIndex], \" has the centrality degree of \", values[highestIndex])\n",
    "\n",
    "    overallAverage = []\n",
    "    # now on to looking at clustering coefficients\n",
    "    for i in range(0, countComps):\n",
    "        clustCoeff = nx.clustering( connectedSubgraphs[i])\n",
    "        coeffVals = list(clustCoeff.values())\n",
    "        overallAverage.append(np.average(coeffVals))\n",
    "    \n",
    "    print (\"Average clustering coefficent is: \", np.average(overallAverage))\n",
    "\n",
    "    ## <--- end of this\n",
    "    \n",
    "    \n",
    "    plt.figure()\n",
    "    nx.draw_networkx(LCC_strong)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "range_limit = 5\n",
    "for n in range(range_limit):\n",
    "    draw_strongly_connected(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## end of strongly connected components\n",
    "- need to comment on interesting not necessarily for the size of the strong components but mostly because of the prominence of their members in other communities - look at that as part of the weakly connected components plus also the community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this returns a list of set of nodes belonging to the \n",
    "# different (weakly) connected components\n",
    "components = list(nx.weakly_connected_components(G))\n",
    "\n",
    "# sort the component according to their size\n",
    "components = list(sorted(components, key=lambda x:len(x), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list with the size of each component\n",
    "comp_sizes = []\n",
    "for comp in components:\n",
    "    comp_sizes.append(len(comp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histogram of component sizes\n",
    "hist = plt.hist(comp_sizes, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram with logarithmic y scale\n",
    "hist = plt.hist(comp_sizes, bins=100, log=True)\n",
    "tx = plt.xlabel('component size')\n",
    "ty = plt.ylabel('number of components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sizes of the ten largest components\n",
    "comp_sizes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make a new graph which is the subgraph of G corresponding to \n",
    "# the largest connected component\n",
    "# let's find the largest component\n",
    "largest_comp = components[2]\n",
    "LCC = G.subgraph(largest_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCC.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the degree distribution inside the LCC\n",
    "degrees = [LCC.degree(n) for n in LCC.nodes()]\n",
    "degrees.sort(reverse=True)\n",
    "degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "nx.draw_networkx(LCC)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "degree_array = np.array(degrees)\n",
    "hist = plt.hist(degree_array, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# using logarithmic scales\n",
    "hist = plt.hist(degree_array, bins=100, log=True)\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logarithmic scale with logarithmic bins\n",
    "N, bins, patches = plt.hist(degree_array, bins=np.logspace(0,np.log10(degree_array.max()+1), 20), log=True)\n",
    "plt.xscale('log')\n",
    "tx = plt.xlabel('k - degree')\n",
    "ty= plt.ylabel('number of nodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree probability distribution (P(k))\n",
    "\n",
    "# since we have logarithmic bins, we need to\n",
    "# take into account the fact that the bins \n",
    "# have different lenghts when normalizing\n",
    "bin_lengths = np.diff(bins) # lenght of each bin\n",
    "\n",
    "summ = np.sum(N*bin_lengths)\n",
    "normalized_degree_dist = N/summ\n",
    "\n",
    "# check normalization:\n",
    "print(np.sum(normalized_degree_dist*bin_lengths))\n",
    "\n",
    "hist = plt.bar(bins[:-1], normalized_degree_dist, width=np.diff(bins))\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "tx = plt.xlabel('k (degree)')\n",
    "ty = plt.ylabel('P(k)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find most important Tweeters using Page Rank\n",
    "PageRank is a generalisation of Google's websearch algorithm, which was originally a method for returning the most important web pages for a given search term. It defined importance as the page which - need to talk about teleporting and surfer stuff - reference this bloke: Gleich, D.F., 2015. PageRank beyond the web. Siam Review, 57(3), pp.321-363.\n",
    "\n",
    "Also then look at from the perspective of centrality as this tells us which nodes(pages) have most edges in or out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#teleportation probability\n",
    "alpha = 0.15\n",
    "\n",
    "#adjacency matrix\n",
    "nodelist = list(G.nodes())\n",
    "A = nx.to_numpy_array(G, nodelist=nodelist)\n",
    "\n",
    "#diagonal matrix of out degrees\n",
    "deg_out_vect = np.array([float(max(G.out_degree(n),1)) for n in nodelist])\n",
    "D_out_inv = np.diag(1/deg_out_vect)\n",
    "\n",
    "# teleportation transition matrix\n",
    "N = A.shape[1]\n",
    "S = np.ones((N,N))*1/N\n",
    "\n",
    "# full transition matrix\n",
    "M = (1-alpha)*D_out_inv @ A + alpha*S\n",
    "\n",
    "# for dangling nodes (nodes without out-edges), we force the random teleportation\n",
    "dangling_nodes = np.where(A.sum(1) == 0)[0]\n",
    "M[dangling_nodes,:] = S[dangling_nodes,:]\n",
    "\n",
    "#initial walker distribution and 1st iteration\n",
    "p_last = np.ones(N)*1/N\n",
    "p = np.matmul(p_last, M)\n",
    "\n",
    "# iterate until sufficient convergence\n",
    "eps = 1.0e-8\n",
    "i = 1\n",
    "while np.linalg.norm(p - p_last, 2) > eps:\n",
    "        p_last = p\n",
    "        p = np.matmul(p, M)\n",
    "        i += 1\n",
    "\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_ranking = np.array(np.argsort(p)[::-1])\n",
    "\n",
    "pagerank_values = p[pg_ranking]\n",
    "nodes_pagerank = [nodelist[r] for r in pg_ranking]\n",
    "nodes_pagerank[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_pagerank = [G.nodes[n]['name'] for n in nodes_pagerank]\n",
    "names_pagerank[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare page rank with degree\n",
    "Top 20 nodes by degree:\n",
    "- ('KoolKat1025', 911)\n",
    "- ('LeoKearse', 629)\n",
    "- ('LeaveEUOfficial', 537)\n",
    "- ('SadiqKhan', 510)\n",
    "- ('MayorofLondon', 376)\n",
    "- ('BrexitBassist', 317)\n",
    "- ('PoliticsJOE_UK', 314)\n",
    "- ('mariannaspring', 294)\n",
    "- ('PoliticsForAlI', 240)\n",
    "- ('LBC', 235)\n",
    "- ('PrisonPlanet', 230)\n",
    "- ('NKrankie', 219)\n",
    "- ('talkRADIO', 210)\n",
    "- ('TJ_Knight', 192)\n",
    "- ('metpoliceuk', 171)\n",
    "- ('ashindestad', 170)\n",
    "- ('MickeyD44314901', 160)\n",
    "- ('DJBURNS_was', 160)\n",
    "- ('Independent', 160)\n",
    "- ('standardnews', 152)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = plt.bar(np.arange(p.shape[0]),np.sort(p)[::-1])\n",
    "ty = plt.ylabel('PageRank value')\n",
    "tx = plt.xlabel('PageRank ranking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pagerank is a probability density\n",
    "pagerank_values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the network of the top 5 nodes\n",
    "plt.figure(figsize=(10,10))\n",
    "nx.draw(G, nodelist=nodes_pagerank[:5], node_size=8000*pagerank_values[:5],width=0.5, arrows=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's add the pagerank value as a node attribute\n",
    "for n, pr in zip(nodes_pagerank,pagerank_values):\n",
    "    if n in LCC:\n",
    "        LCC.nodes[n]['page_rank'] = pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_pagerank[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
