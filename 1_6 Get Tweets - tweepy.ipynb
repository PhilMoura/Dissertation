{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Tweets\n",
    "## Tools\n",
    "We set up a Twitter developer account and attempted to use the Tweepy tool to extract tweets. Our method was influenced by \n",
    "https://www.earthdatascience.org/courses/use-data-open-source-python/intro-to-apis/twitter-data-in-python/\n",
    "\n",
    "We did extract data but we encountered problems when trying to access historic data.\n",
    "\n",
    "## Problems encountered\n",
    "\n",
    "Unfortunately, the Twitter API only returns tweets from the most recent week, irrespective of which start date one provides. This means that we will have to run the extract on a weekly basis (to get the prior week's tweets) in order to get a reasonable body of tweets. This is suboptimal and could restrict our ability to go back and query prior results.\n",
    "\n",
    "I addition, running search queries on Twitter takes a lot of time and can get timed out. For example, see Appendix A for messages received when I timed out searching Twitter using the search term \"sadiq AND khan\". Previously this terms had taken 3 hours to complete and returned approximately 12,000 rows. However, it has then consistently failed due to timeouts. I am not sure whether this is happenstance or whether Twitter have throttled my ability to download large volumes of Twitter data, given I had recently downloaded this data. This is of interest to other researchers using developer accounts. \n",
    "\n",
    "## Twitter research account\n",
    "I therefore applied to Twitter's Academic Research track https://developer.twitter.com/en/products/twitter-api/academic-research because this allows researchers to access historic tweets, and in higher tweet volumes. My requests were rejected and I discuss this in further detail in Appendix B. The reason I discuss this is because researching Twitter data without academic access is difficult and yet getting academic access whilst not having a presence on University websites does not seem possible \n",
    "\n",
    "## Tweepy Code References:\n",
    "- retweet and favourite counts, better dataframe creator using Tweepy - https://towardsdatascience.com/how-to-build-a-dataset-from-twitter-using-python-tweepy-861bdbc16fa5\n",
    "- Getting user and location - https://stackoverflow.com/questions/50366489/how-to-get-twitter-users-screen-name-or-userid-from-a-specific-geolocations\n",
    "- Cleaning tweet text and finding out if retweet - https://stackoverflow.com/questions/50052330/tweepy-check-if-a-tweet-is-a-retweet\n",
    "- geocordinates - https://stackoverflow.com/questions/46044445/not-able-to-scrape-geo-coordinate-with-tweets-lat-lon\n",
    "- avoiding twitter api rate limit - https://stackoverflow.com/questions/21308762/avoid-twitter-api-limitation-with-tweepy\n",
    "- keeping authentication details secret - https://www.digitalocean.com/community/tutorials/how-to-create-a-twitterbot-with-python-3-and-the-tweepy-library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tweepy\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get Twitter Data\n",
    "We have two choices to loading twitter data:\n",
    "- 1.1. use the Tweepy API (but this can take hours)\n",
    "- 1.2. load the previously saved Twitter data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load data from Twitter \n",
    "#### 1.1.1 Twitter credentials file\n",
    "I don't want to make my Twitter credentials public and so these are loaded from a credentials file and that file is not uploaded to github. \n",
    "\n",
    "To replicate this code, create a 'credentials.py' file with the following lines (using your own credential details):\n",
    "\n",
    "`\n",
    "consumer_key = 'your_consumer_key'\n",
    "consumer_secret = 'your_consumer_secret'\n",
    "access_token = 'your_access_token'\n",
    "access_token_secret = 'your_access_token_secret'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from credentials import *\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1.1.2 Set date parameters\n",
    "Using search words, I want to get all tweets between today's date and a start date of July 1, 2019\n",
    "- The start date is just before the day the Mayor made his speech and today's date is used so I can collect as many tweets as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./DataSources/TwitterData/raw_tweets_20210730.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(datetime.date(2019, 7, 1), '20210730')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_from = datetime.date(2019, 7, 1) # this doesn't actually work as twitter only goes back one week\n",
    "today = datetime.datetime.today().strftime('%Y%m%d')\n",
    "\n",
    "outputfile_str = \"./DataSources/TwitterData/raw_tweets_\" + today + \".csv\"\n",
    "print(outputfile_str)\n",
    "\n",
    "date_from, today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 Get tweets using a cursor\n",
    "- First we define our function to load tweets\n",
    "- Create search terms to query Twitter - return results as a list of dictionary items\n",
    "- Concatenate all returned results and then use this to create a pandas dataframe\n",
    "\n",
    "##### 1.1.3.1 define get_tweets\n",
    "In order to process quote tweets we borrowed code from https://blog.f-secure.com/processing-quote-tweets-with-twitter-api/ and then, because it didn't work well, searched on json.dumps and json.loads to work out how to strip out the json string for the quoted user and then to turn that into a dictionary I could easily interrogate\n",
    "- error processing purloined from https://stackoverflow.com/questions/27351207/gracefully-handle-errors-and-exceptions-for-user-timeline-method-in-tweepy\n",
    "    - error code 50 means there isn't a user for this user id\n",
    "    - error code 63 means this user id refers to a user who has been suspended from Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "def get_tweets(search_words, my_api, today): \n",
    "    tic = time.perf_counter()\n",
    "    tweets = tweepy.Cursor(my_api.search,\n",
    "                       q=search_words,\n",
    "                       lang=\"en\",\n",
    "                       since=date_from).items()\n",
    "    \n",
    "    output = []\n",
    "    for tweet in tweets:\n",
    "               \n",
    "        try:\n",
    "            tweet_id = tweet.id\n",
    "            text = tweet.text\n",
    "            tweet_date = tweet.created_at\n",
    "            user_id_str = tweet.user.id_str\n",
    "            screen_name = tweet.user.screen_name\n",
    "            user_name = tweet.user.name\n",
    "            user_id = api.get_user(user_id_str)\n",
    "        \n",
    "            in_reply_to_user_screen_name = \"\"\n",
    "            quote_tweet_screen_name = \"\"\n",
    "            \n",
    "            if tweet.in_reply_to_user_id is not None: \n",
    "                in_reply_to_user_id = tweet.in_reply_to_user_id \n",
    "                in_reply_to_user_screen_name = api.get_user(in_reply_to_user_id).screen_name\n",
    "             \n",
    "            if hasattr(tweet, 'quoted_status'): \n",
    "                quote_tweet = tweet.quoted_status            \n",
    "                quote_tweet_str = json.dumps(quote_tweet._json) # dumps json component into a string\n",
    "                quote_tweet_dict = json.loads(quote_tweet_str) # loads the string into a dictionary                       \n",
    "                quote_tweet_id = quote_tweet_dict[\"user\"][\"id\"]\n",
    "                quote_tweet_screen_name = api.get_user(quote_tweet_id).screen_name\n",
    "                                  \n",
    "            user_location = user_id.location\n",
    "            user_coordinates = tweet.coordinates\n",
    "            favourite_count = tweet.favorite_count\n",
    "            retweet_count = tweet.retweet_count\n",
    "                \n",
    "            line = {'tweet_id' : tweet_id,\n",
    "                'tweet_date' : tweet_date,\n",
    "                'tweeter_id' : user_id_str,\n",
    "                'tweeter_user_name' : user_name,\n",
    "                'tweeter_screen_name' : screen_name,\n",
    "                'tweeter_location' : user_location,\n",
    "                'tweeter_coordinates' : user_coordinates,\n",
    "                'message_text' : text,\n",
    "                'in_reply_to_user_screen_name' : in_reply_to_user_screen_name,      \n",
    "                'quote_tweet_screen_name' : quote_tweet_screen_name,\n",
    "                'favourite_count' : favourite_count, \n",
    "                'retweet_count' : retweet_count,\n",
    "                'extract_run_date' : today,\n",
    "                'retrieved_using_search_term' : search_words}\n",
    "            output.append(line)\n",
    "        \n",
    "        except tweepy.TweepError as e:\n",
    "            print('\\n **************** error ***************')\n",
    "            print(e)\n",
    "            print('\\n ********* end of error text **********')\n",
    "               \n",
    "    toc = time.perf_counter()\n",
    "    time_taken = toc - tic\n",
    "    \n",
    "    print('Time taken to process search term : {} , was {:.2f}'.format(search_words, time_taken))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.3.2 create list of search terms and iteratively get tweets using these terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 678\n",
      "Rate limit reached. Sleeping for: 682\n",
      "Rate limit reached. Sleeping for: 674\n",
      "Rate limit reached. Sleeping for: 670\n",
      "Rate limit reached. Sleeping for: 660\n",
      "Rate limit reached. Sleeping for: 648\n",
      "Rate limit reached. Sleeping for: 670\n",
      "Rate limit reached. Sleeping for: 638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " **************** error ***************\n",
      "Failed to send request: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "\n",
      " ********* end of error text **********\n",
      "\n",
      " **************** error ***************\n",
      "[{'code': 63, 'message': 'User has been suspended.'}]\n",
      "\n",
      " ********* end of error text **********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " **************** error ***************\n",
      "[{'code': 50, 'message': 'User not found.'}]\n",
      "\n",
      " ********* end of error text **********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " **************** error ***************\n",
      "Failed to send request: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "\n",
      " ********* end of error text **********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " **************** error ***************\n",
      "[{'code': 50, 'message': 'User not found.'}]\n",
      "\n",
      " ********* end of error text **********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " **************** error ***************\n",
      "Failed to send request: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "\n",
      " ********* end of error text **********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " **************** error ***************\n",
      "[{'code': 50, 'message': 'User not found.'}]\n",
      "\n",
      " ********* end of error text **********\n",
      "Time taken to process search term : sadiq AND khan , was 47091.57\n",
      "Time taken to process ALL search terms : ['sadiq AND khan'] , was 47091.65\n"
     ]
    }
   ],
   "source": [
    "search_terms = [\"London AND knife AND crime\",\n",
    "                \"London AND knifecrime\",\n",
    "                \"Khan AND knife AND crime\",\n",
    "                \"Khan AND knifecrime\",\n",
    "                \"London AND violent AND crime\",\n",
    "                \"youth AND violent AND crime\",\n",
    "                \"youth AND crime AND London\"\n",
    "                \"youth AND knife AND crime\",\n",
    "                \"london AND youthcrime\",\n",
    "                \"#knifecrime AND #khan\",\n",
    "                \"#knifecrime AND #london\",\n",
    "                \"#violence AND #khan\",\n",
    "                \"#london AND #youthcrime\",\n",
    "                \"London AND crime\",\n",
    "                \"London AND stabbing\"]\n",
    "\n",
    "# The following term was queried on 30/07/2021 to get wider context on what's being tweeted\n",
    "#search_terms = [\"sadiq AND khan\"]\n",
    "\n",
    "all_tweets = []\n",
    "\n",
    "tic = time.perf_counter()\n",
    "\n",
    "for search_term in search_terms:\n",
    "    current_tweets = get_tweets(search_term, api, today)\n",
    "    all_tweets.append(current_tweets)\n",
    "\n",
    "toc = time.perf_counter()\n",
    "time_taken = toc - tic\n",
    "    \n",
    "print('Time taken to process ALL search terms : {} , was {:.2f}'.format(search_terms, time_taken))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4 create all_tweets_df dataframe\n",
    "First check on how many items downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tweets in current list = 11607\n",
      "(11607, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>tweeter_id</th>\n",
       "      <th>tweeter_user_name</th>\n",
       "      <th>tweeter_screen_name</th>\n",
       "      <th>tweeter_location</th>\n",
       "      <th>tweeter_coordinates</th>\n",
       "      <th>message_text</th>\n",
       "      <th>in_reply_to_user_screen_name</th>\n",
       "      <th>quote_tweet_screen_name</th>\n",
       "      <th>favourite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>extract_run_date</th>\n",
       "      <th>retrieved_using_search_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1421147981890347008</td>\n",
       "      <td>2021-07-30 16:37:37</td>\n",
       "      <td>739449695957884928</td>\n",
       "      <td>Tommy Brexit تومي بركزت</td>\n",
       "      <td>Brexit4us</td>\n",
       "      <td>London, England</td>\n",
       "      <td>None</td>\n",
       "      <td>RT @LTHlondon: Gridlock, gridlock and more GRI...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20210730</td>\n",
       "      <td>sadiq AND khan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1421147788851793928</td>\n",
       "      <td>2021-07-30 16:36:51</td>\n",
       "      <td>215217791</td>\n",
       "      <td>David</td>\n",
       "      <td>WeeksyD</td>\n",
       "      <td>South Wales</td>\n",
       "      <td>None</td>\n",
       "      <td>RT @torysleazeUK: https://t.co/UcoFsOGkwb🏴‍☠️ ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>20210730</td>\n",
       "      <td>sadiq AND khan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1421147566780059655</td>\n",
       "      <td>2021-07-30 16:35:58</td>\n",
       "      <td>1132230860</td>\n",
       "      <td>Annie Artist 💙</td>\n",
       "      <td>Artyannie</td>\n",
       "      <td>South Lanarkshire Scotland</td>\n",
       "      <td>None</td>\n",
       "      <td>RT @GoodLawProject: BREAKING: New emails revea...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>4285</td>\n",
       "      <td>20210730</td>\n",
       "      <td>sadiq AND khan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1421146682201411587</td>\n",
       "      <td>2021-07-30 16:32:27</td>\n",
       "      <td>19291485</td>\n",
       "      <td>LTH🇬🇧london</td>\n",
       "      <td>LTHlondon</td>\n",
       "      <td>London, England</td>\n",
       "      <td>None</td>\n",
       "      <td>Gridlock, gridlock and more GRIDLOCK! For as f...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20210730</td>\n",
       "      <td>sadiq AND khan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1421146614907932675</td>\n",
       "      <td>2021-07-30 16:32:11</td>\n",
       "      <td>442906117</td>\n",
       "      <td>Alistair James Baker</td>\n",
       "      <td>Albaker1984</td>\n",
       "      <td>London</td>\n",
       "      <td>None</td>\n",
       "      <td>RT @LBC: 'I pay £15 a day to drive 160 yards.'...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>20210730</td>\n",
       "      <td>sadiq AND khan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id          tweet_date          tweeter_id  \\\n",
       "0  1421147981890347008 2021-07-30 16:37:37  739449695957884928   \n",
       "1  1421147788851793928 2021-07-30 16:36:51           215217791   \n",
       "2  1421147566780059655 2021-07-30 16:35:58          1132230860   \n",
       "3  1421146682201411587 2021-07-30 16:32:27            19291485   \n",
       "4  1421146614907932675 2021-07-30 16:32:11           442906117   \n",
       "\n",
       "         tweeter_user_name tweeter_screen_name            tweeter_location  \\\n",
       "0  Tommy Brexit تومي بركزت           Brexit4us             London, England   \n",
       "1                    David             WeeksyD                 South Wales   \n",
       "2           Annie Artist 💙           Artyannie  South Lanarkshire Scotland   \n",
       "3              LTH🇬🇧london           LTHlondon             London, England   \n",
       "4     Alistair James Baker         Albaker1984                      London   \n",
       "\n",
       "  tweeter_coordinates                                       message_text  \\\n",
       "0                None  RT @LTHlondon: Gridlock, gridlock and more GRI...   \n",
       "1                None  RT @torysleazeUK: https://t.co/UcoFsOGkwb🏴‍☠️ ...   \n",
       "2                None  RT @GoodLawProject: BREAKING: New emails revea...   \n",
       "3                None  Gridlock, gridlock and more GRIDLOCK! For as f...   \n",
       "4                None  RT @LBC: 'I pay £15 a day to drive 160 yards.'...   \n",
       "\n",
       "  in_reply_to_user_screen_name quote_tweet_screen_name favourite_count  \\\n",
       "0                                                                    0   \n",
       "1                                                                    0   \n",
       "2                                                                    0   \n",
       "3                                                                    1   \n",
       "4                                                                    0   \n",
       "\n",
       "  retweet_count extract_run_date retrieved_using_search_term  \n",
       "0             1         20210730              sadiq AND khan  \n",
       "1            12         20210730              sadiq AND khan  \n",
       "2          4285         20210730              sadiq AND khan  \n",
       "3             1         20210730              sadiq AND khan  \n",
       "4            11         20210730              sadiq AND khan  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets_df = pd.DataFrame(columns=['tweet_id', \n",
    "                                      'tweet_date', \n",
    "                                      'tweeter_id', \n",
    "                                      'tweeter_user_name', \n",
    "                                      'tweeter_screen_name', \n",
    "                                      'tweeter_location',\n",
    "                                      'tweeter_coordinates',\n",
    "                                      'message_text',\n",
    "                                      'in_reply_to_user_screen_name', \n",
    "                                      'quote_tweet_screen_name',\n",
    "                                      'favourite_count',\n",
    "                                      'retweet_count',\n",
    "                                      'extract_run_date',\n",
    "                                      'retrieved_using_search_term'])\n",
    "\n",
    "for these_tweets in all_tweets:\n",
    "    print('number of tweets in current list = {}'.format(len(these_tweets)))\n",
    "\n",
    "    df_tweets = pd.DataFrame(these_tweets)\n",
    "    all_tweets_df = all_tweets_df.append(df_tweets, ignore_index=True)\n",
    "\n",
    "print(all_tweets_df.shape)\n",
    "\n",
    "all_tweets_df.to_csv(outputfile_str, index=False)\n",
    "\n",
    "all_tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Load previously saved Twitter data\n",
    "- Need to change the file name passed to 'load_file_name' if we want a prior dataset\n",
    "- Will eventually concatenate all the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11607, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>tweeter_id</th>\n",
       "      <th>tweeter_user_name</th>\n",
       "      <th>tweeter_screen_name</th>\n",
       "      <th>tweeter_location</th>\n",
       "      <th>tweeter_coordinates</th>\n",
       "      <th>message_text</th>\n",
       "      <th>in_reply_to_user_screen_name</th>\n",
       "      <th>quote_tweet_screen_name</th>\n",
       "      <th>favourite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>extract_run_date</th>\n",
       "      <th>retrieved_using_search_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1421147981890347008</td>\n",
       "      <td>2021-07-30 16:37:37</td>\n",
       "      <td>739449695957884928</td>\n",
       "      <td>Tommy Brexit تومي بركزت</td>\n",
       "      <td>Brexit4us</td>\n",
       "      <td>London, England</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @LTHlondon: Gridlock, gridlock and more GRI...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20210730</td>\n",
       "      <td>sadiq AND khan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1421147788851793928</td>\n",
       "      <td>2021-07-30 16:36:51</td>\n",
       "      <td>215217791</td>\n",
       "      <td>David</td>\n",
       "      <td>WeeksyD</td>\n",
       "      <td>South Wales</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @torysleazeUK: https://t.co/UcoFsOGkwb🏴‍☠️ ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>20210730</td>\n",
       "      <td>sadiq AND khan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1421147566780059655</td>\n",
       "      <td>2021-07-30 16:35:58</td>\n",
       "      <td>1132230860</td>\n",
       "      <td>Annie Artist 💙</td>\n",
       "      <td>Artyannie</td>\n",
       "      <td>South Lanarkshire Scotland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @GoodLawProject: BREAKING: New emails revea...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4285</td>\n",
       "      <td>20210730</td>\n",
       "      <td>sadiq AND khan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1421146682201411587</td>\n",
       "      <td>2021-07-30 16:32:27</td>\n",
       "      <td>19291485</td>\n",
       "      <td>LTH🇬🇧london</td>\n",
       "      <td>LTHlondon</td>\n",
       "      <td>London, England</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gridlock, gridlock and more GRIDLOCK! For as f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20210730</td>\n",
       "      <td>sadiq AND khan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1421146614907932675</td>\n",
       "      <td>2021-07-30 16:32:11</td>\n",
       "      <td>442906117</td>\n",
       "      <td>Alistair James Baker</td>\n",
       "      <td>Albaker1984</td>\n",
       "      <td>London</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @LBC: 'I pay £15 a day to drive 160 yards.'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>20210730</td>\n",
       "      <td>sadiq AND khan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id           tweet_date          tweeter_id  \\\n",
       "0  1421147981890347008  2021-07-30 16:37:37  739449695957884928   \n",
       "1  1421147788851793928  2021-07-30 16:36:51           215217791   \n",
       "2  1421147566780059655  2021-07-30 16:35:58          1132230860   \n",
       "3  1421146682201411587  2021-07-30 16:32:27            19291485   \n",
       "4  1421146614907932675  2021-07-30 16:32:11           442906117   \n",
       "\n",
       "         tweeter_user_name tweeter_screen_name            tweeter_location  \\\n",
       "0  Tommy Brexit تومي بركزت           Brexit4us             London, England   \n",
       "1                    David             WeeksyD                 South Wales   \n",
       "2           Annie Artist 💙           Artyannie  South Lanarkshire Scotland   \n",
       "3              LTH🇬🇧london           LTHlondon             London, England   \n",
       "4     Alistair James Baker         Albaker1984                      London   \n",
       "\n",
       "   tweeter_coordinates                                       message_text  \\\n",
       "0                  NaN  RT @LTHlondon: Gridlock, gridlock and more GRI...   \n",
       "1                  NaN  RT @torysleazeUK: https://t.co/UcoFsOGkwb🏴‍☠️ ...   \n",
       "2                  NaN  RT @GoodLawProject: BREAKING: New emails revea...   \n",
       "3                  NaN  Gridlock, gridlock and more GRIDLOCK! For as f...   \n",
       "4                  NaN  RT @LBC: 'I pay £15 a day to drive 160 yards.'...   \n",
       "\n",
       "  in_reply_to_user_screen_name quote_tweet_screen_name  favourite_count  \\\n",
       "0                          NaN                     NaN                0   \n",
       "1                          NaN                     NaN                0   \n",
       "2                          NaN                     NaN                0   \n",
       "3                          NaN                     NaN                1   \n",
       "4                          NaN                     NaN                0   \n",
       "\n",
       "   retweet_count  extract_run_date retrieved_using_search_term  \n",
       "0              1          20210730              sadiq AND khan  \n",
       "1             12          20210730              sadiq AND khan  \n",
       "2           4285          20210730              sadiq AND khan  \n",
       "3              1          20210730              sadiq AND khan  \n",
       "4             11          20210730              sadiq AND khan  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_file_name = outputfile_str\n",
    "\n",
    "all_tweets_df_new = pd.read_csv(load_file_name)\n",
    "print(all_tweets_df_new.shape)\n",
    "all_tweets_df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A - Twitter Timeout error\n",
    "The following error message was received when extracting data using the search term \"Sadiq AND Khan\"\n",
    "\n",
    "### error message\n",
    "TimeoutError                              Traceback (most recent call last)\n",
    "~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py in _new_conn(self)\n",
    "    158         try:\n",
    "--> 159             conn = connection.create_connection(\n",
    "    160                 (self._dns_host, self.port), self.timeout, **extra_kw\n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py in create_connection(address, timeout, source_address, socket_options)\n",
    "     83     if err is not None:\n",
    "---> 84         raise err\n",
    "     85 \n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py in create_connection(address, timeout, source_address, socket_options)\n",
    "     73                 sock.bind(source_address)\n",
    "---> 74             sock.connect(sa)\n",
    "     75             return sock\n",
    "\n",
    "TimeoutError: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond\n",
    "\n",
    "During handling of the above exception, another exception occurred:\n",
    "\n",
    "NewConnectionError                        Traceback (most recent call last)\n",
    "~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\n",
    "    669             # Make the request on the httplib connection object.\n",
    "--> 670             httplib_response = self._make_request(\n",
    "    671                 conn,\n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)\n",
    "    380         try:\n",
    "--> 381             self._validate_conn(conn)\n",
    "    382         except (SocketTimeout, BaseSSLError) as e:\n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py in _validate_conn(self, conn)\n",
    "    975         if not getattr(conn, \"sock\", None):  # AppEngine might not have  `.sock`\n",
    "--> 976             conn.connect()\n",
    "    977 \n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py in connect(self)\n",
    "    307         # Add certificate verification\n",
    "--> 308         conn = self._new_conn()\n",
    "    309         hostname = self.host\n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py in _new_conn(self)\n",
    "    170         except SocketError as e:\n",
    "--> 171             raise NewConnectionError(\n",
    "    172                 self, \"Failed to establish a new connection: %s\" % e\n",
    "\n",
    "NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000183218E7B50>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond\n",
    "\n",
    "During handling of the above exception, another exception occurred:\n",
    "\n",
    "MaxRetryError                             Traceback (most recent call last)\n",
    "~\\anaconda3\\lib\\site-packages\\requests\\adapters.py in send(self, request, stream, timeout, verify, cert, proxies)\n",
    "    438             if not chunked:\n",
    "--> 439                 resp = conn.urlopen(\n",
    "    440                     method=request.method,\n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\n",
    "    723 \n",
    "--> 724             retries = retries.increment(\n",
    "    725                 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py in increment(self, method, url, response, error, _pool, _stacktrace)\n",
    "    438         if new_retry.is_exhausted():\n",
    "--> 439             raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
    "    440 \n",
    "\n",
    "MaxRetryError: HTTPSConnectionPool(host='api.twitter.com', port=443): Max retries exceeded with url: /1.1/users/show.json?id=862885892 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000183218E7B50>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond'))\n",
    "\n",
    "During handling of the above exception, another exception occurred:\n",
    "\n",
    "ConnectionError                           Traceback (most recent call last)\n",
    "~\\anaconda3\\lib\\site-packages\\tweepy\\binder.py in execute(self)\n",
    "    183                 try:\n",
    "--> 184                     resp = self.session.request(self.method,\n",
    "    185                                                 full_url,\n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\requests\\sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\n",
    "    529         send_kwargs.update(settings)\n",
    "--> 530         resp = self.send(prep, **send_kwargs)\n",
    "    531 \n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\requests\\sessions.py in send(self, request, **kwargs)\n",
    "    642         # Send the request\n",
    "--> 643         r = adapter.send(request, **kwargs)\n",
    "    644 \n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\requests\\adapters.py in send(self, request, stream, timeout, verify, cert, proxies)\n",
    "    515 \n",
    "--> 516             raise ConnectionError(e, request=request)\n",
    "    517 \n",
    "\n",
    "ConnectionError: HTTPSConnectionPool(host='api.twitter.com', port=443): Max retries exceeded with url: /1.1/users/show.json?id=862885892 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000183218E7B50>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond'))\n",
    "\n",
    "During handling of the above exception, another exception occurred:\n",
    "\n",
    "TweepError                                Traceback (most recent call last)\n",
    "<ipython-input-7-c24d3d2b3d48> in <module>\n",
    "     18 \n",
    "     19 for search_term in search_terms:\n",
    "---> 20     current_tweets = get_tweets(search_term, api, today)\n",
    "     21     all_tweets.append(current_tweets)\n",
    "     22 \n",
    "\n",
    "<ipython-input-4-92b12659b409> in get_tweets(search_words, my_api, today)\n",
    "     16         screen_name = tweet.user.screen_name\n",
    "     17         user_name = tweet.user.name\n",
    "---> 18         user_id = api.get_user(user_id_str)\n",
    "     19         user_location = user_id.location\n",
    "     20         user_coordinates = tweet.coordinates\n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\tweepy\\binder.py in _call(*args, **kwargs)\n",
    "    251                 return method\n",
    "    252             else:\n",
    "--> 253                 return method.execute()\n",
    "    254         finally:\n",
    "    255             method.session.close()\n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\tweepy\\binder.py in execute(self)\n",
    "    190                                                 proxies=self.api.proxy)\n",
    "    191                 except Exception as e:\n",
    "--> 192                     six.reraise(TweepError, TweepError('Failed to send request: %s' % e), sys.exc_info()[2])\n",
    "    193 \n",
    "    194                 rem_calls = resp.headers.get('x-rate-limit-remaining')\n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\six.py in reraise(tp, value, tb)\n",
    "    700                 value = tp()\n",
    "    701             if value.__traceback__ is not tb:\n",
    "--> 702                 raise value.with_traceback(tb)\n",
    "    703             raise value\n",
    "    704         finally:\n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\tweepy\\binder.py in execute(self)\n",
    "    182                 # Execute request\n",
    "    183                 try:\n",
    "--> 184                     resp = self.session.request(self.method,\n",
    "    185                                                 full_url,\n",
    "    186                                                 data=self.post_data,\n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\requests\\sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\n",
    "    528         }\n",
    "    529         send_kwargs.update(settings)\n",
    "--> 530         resp = self.send(prep, **send_kwargs)\n",
    "    531 \n",
    "    532         return resp\n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\requests\\sessions.py in send(self, request, **kwargs)\n",
    "    641 \n",
    "    642         # Send the request\n",
    "--> 643         r = adapter.send(request, **kwargs)\n",
    "    644 \n",
    "    645         # Total elapsed time of the request (approximately)\n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\requests\\adapters.py in send(self, request, stream, timeout, verify, cert, proxies)\n",
    "    514                 raise SSLError(e, request=request)\n",
    "    515 \n",
    "--> 516             raise ConnectionError(e, request=request)\n",
    "    517 \n",
    "    518         except ClosedPoolError as e:\n",
    "\n",
    "TweepError: Failed to send request: HTTPSConnectionPool(host='api.twitter.com', port=443): Max retries exceeded with url: /1.1/users/show.json?id=862885892 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000183218E7B50>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix B - Twitter academic research access\n",
    "As discussed, I applied for the research track as this offers access to historical twitter data and also the ability to download higher volumes of Tweets. Unfortunately my access requests were rejected.\n",
    "\n",
    "My original request failed, with Twitter responding that I did not meet their use case for a research account. I believe this is because they wanted to be able to reference my name via an official University website, for example within a Student directory. However, City University do not make student directories publicly available (for entirely understandable reasons), which suggests the research account isn't readily available to students and it is more aimed at researchers and faculty members who are referencable via the University website. I then reapplied using my city email address and having set up a Twitter account linked to this email address. I am waiting for a response (25/07/2021).\n",
    "- I received a response on 25/07/2021 saying my request did not \"qualify for academic access to the Twitter API\". Twitter do not give specific reasons and so it's not possible to understand whether they do not give access if they cannot identify students on a university directory or whether this specific research falls outside what they consider acceptable research (although they did say it qualified for regular developer access, which suggests the research topic was OK).\n",
    "\n",
    "I any case I go into detail on the application process because not having access to historic tweets significantly impacts our ability to perform the desired research and the process to get access is time consuming, opaque and there is no right to appeal. It would discourage me from doing further academic research into Twitter data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
