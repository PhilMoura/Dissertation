{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social networks\n",
    "We have a body of tweets from which we can understand who is communication with who and by extension which communities exist within our body of tweeters. We also have a set of tweeter screen names, labeled to express whether they hold an opinion 'AGAINST' or 'NOT AGAINST' the London Mayor on the causes of crime. In this notebook we will use this data \n",
    "\n",
    "## Context\n",
    "Previously we analysed Twitter data to understand 'WHAT' was being discussed about serious violent crime on Twitter and used that to predict whether the opinion expressed within the tweet was 'AGAINST' or 'NOT AGAINST' the causes given by the Mayor. In this notebook we focus on the 'WHO', which is the communities that are discussing crime and the key influencers within these communities. Having identified these communities we can then see how which of these communities tweeters we previously labeled as against belong to and whether there is any relationship. This will help us answer our sec\n",
    "\n",
    "\n",
    "\n",
    "This is the first step in helping us answer our research question\n",
    "- Research Question 1 (RQ1): Can Twitter sentiment analysis determine the proportions of Twitter users that accept or reject the London Mayor’s evidence that ‘deprivation in the leading cause of youth violent crime in London’?\n",
    "- Research Question 2 (RQ2): Can this approach additionally identify the social groups to which users rejecting the Mayor’s evidence belong, and whether this view is widely spread within those groups?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "we were going to do membership over time but that was on the expectation we could get twitter data spanning 2 years. We don't believe this is beneficial given we only have data for 1 month.\n",
    "\n",
    "Add as a conclusion:\n",
    "Strongly connected networks are two way conversations but previously we saw 75% of tweets were retweets and so this suggests the networks are far more one way and so that's why we get more useful results using weakly connected networks.\n",
    "\n",
    "\n",
    "\n",
    "I basically use two sources for this, Bovet and programminghistorian\n",
    "\n",
    "What I want to find out is basically the following:\n",
    "1. Are there obvious social clusters within our network\n",
    "2. If so, how big are they and how strong are the relationships\n",
    "3. Within these networks, who are the most important tweeters\n",
    "\n",
    "We also want to know the following general information:\n",
    "- Which users are most important, irrespective of network\n",
    "\n",
    "## Changes I need to make\n",
    "- Bring strong connections up front - say not really informative but keep an eye out for key players\n",
    "- Then do overview of degrees and centrality - explain why\n",
    "- Get components + communities\n",
    "- Do page ranking - explain why and compare with degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = pd.read_csv(\"./DataSources/TwitterData/cleaned_tweets.csv\")\n",
    "print(all_tweets.shape)\n",
    "all_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = all_tweets.dropna(how='all') # only drops a row when every column is NA\n",
    "print(\"shape before after dropping rows with all NaN\")\n",
    "print(all_tweets.shape)\n",
    "\n",
    "# Now check for individual NaN values\n",
    "nan_values = all_tweets[all_tweets.isna().any(axis=1)]\n",
    "print(nan_values.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to set the in_reply_to_user_screen_name and quote_tweet_screen_name fields to blanks\n",
    "all_tweets.loc[all_tweets['in_reply_to_user_screen_name'].isna(), 'in_reply_to_user_screen_name'] = ''\n",
    "all_tweets.loc[all_tweets['quote_tweet_screen_name'].isna(), 'quote_tweet_screen_name'] = ''\n",
    "\n",
    "# Now check for individual NaN values\n",
    "nan_values = all_tweets[all_tweets.isna().any(axis=1)]\n",
    "print(nan_values.count())\n",
    "\n",
    "nan_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network of interactions\n",
    "### This code draws heavily on the work of Bovet\n",
    "### https://github.com/alexbovet/network_lesson/blob/master/02_Analysis_of_Twitter_Social_Network.ipynb\n",
    "\n",
    "We will use the python module NetworkX to construct and analyze the social network.\n",
    "\n",
    "There are four types of interactions between two users in Twitter:\n",
    "- Retweet\n",
    "- Quote\n",
    "- Reply\n",
    "- Mention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define some functions to extract the interactions from tweets\n",
    "def string_to_list(my_str):\n",
    "    delimiter = \",\"\n",
    "    my_str = my_str.replace(\"[\", \"\")\n",
    "    my_str = my_str.replace(\"]\", \"\")\n",
    "    my_str = my_str.replace(\"@\", \"\")\n",
    "    my_str = my_str.replace(\"'\", \"\")\n",
    "    my_str = my_str.replace(\" \", \"\")\n",
    "    my_list = my_str.split(delimiter)\n",
    "    return my_list\n",
    "\n",
    "def getAllInteractions(tweet):\n",
    "    \n",
    "    # Get the tweeter\n",
    "    tweet_id = tweet.tweet_id\n",
    "    tweeter_id = tweet.tweeter_id\n",
    "    tweeter_name = tweet.tweeter_screen_name\n",
    "    \n",
    "    # a python set is a collection of unique items\n",
    "    # we use a set to avoid duplicated ids\n",
    "    interacting_users = set()\n",
    "    \n",
    "    # Add person they're replying to\n",
    "    if tweet.in_reply_to_user_screen_name != '':\n",
    "        interacting_users.add(tweet.in_reply_to_user_screen_name)\n",
    "        \n",
    "    # Add person they quoted\n",
    "    if tweet.quote_tweet_screen_name != '':\n",
    "        interacting_users.add(tweet.quote_tweet_screen_name)\n",
    "    \n",
    "    # Add person they retweeted\n",
    "    if len(tweet.retweeted) > 2: # because empty strings will contain []\n",
    "        retweeted_list = string_to_list(tweet.retweeted)\n",
    "        for item in retweeted_list:\n",
    "            interacting_users.add(item)\n",
    "       \n",
    "    # Add mentions\n",
    "    if len(tweet.mentioned) > 2: # because empty strings will contain []\n",
    "        mentioned_list = string_to_list(tweet.mentioned)\n",
    "        for item in mentioned_list:\n",
    "            interacting_users.add(item)\n",
    "  \n",
    "    # remove the tweeter if he is in the set\n",
    "    interacting_users.discard(tweeter_name)\n",
    "    \n",
    "    # Return our tweeter and their influencers\n",
    "    return tweeter_id, tweeter_name, tweet_id, list(interacting_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# define an empty Directed Graph\n",
    "# A directed graph is a graph where edges have a direction\n",
    "# in our case the edges goes from user that sent the tweet to\n",
    "# the user with whom they interacted (retweeted, mentioned or quoted)\n",
    "G = nx.DiGraph()\n",
    "\n",
    "for index, tweet in all_tweets.iterrows():\n",
    "    \n",
    "    if (tweet.tweeter_screen_name != 'SadiqKhan') & (tweet.tweeter_screen_name != 'MayorofLondon'):\n",
    "        tweeter_id, tweeter_name, tweet_id, interactions = getAllInteractions(tweet)\n",
    "    \n",
    "        # add an edge to the Graph for each influencer\n",
    "        for interact_name in interactions:\n",
    "        \n",
    "            # add edges between the two user ids\n",
    "            # this will create new nodes if the nodes are not already in the network\n",
    "            # we also add an attribute the to edge equal to the id of the tweet\n",
    "            G.add_edge(tweeter_name, interact_name, tweet_id=tweet_id)\n",
    "        \n",
    "            # add name as a property to each node\n",
    "            # with networkX each node is a dictionary\n",
    "            G.nodes[tweeter_name]['name'] = tweeter_name\n",
    "            G.nodes[interact_name]['name'] = interact_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with network characteristics\n",
    "Based on tutorial from: https://programminghistorian.org/en/lessons/exploring-and-analyzing-network-data-with-python \n",
    "- John R. Ladd, Jessica Otis, Christopher N. Warren, and Scott Weingart, \"Exploring and Analyzing Network Data with Python,\" The Programming Historian 6 (2017), https://doi.org/10.46430/phen0064."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On a scale of 0 to 1, where 1 is a dense network\n",
    "density = nx.density(G)\n",
    "print(\"Network density:\", density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_dict = dict(G.degree(G.nodes()))\n",
    "nx.set_node_attributes(G, degree_dict, 'degree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the keys are the user_id\n",
    "nodelist = list(G.nodes.keys())\n",
    "print(nodelist[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each node is itself a dictionary with node attributes as key,value pairs\n",
    "print(type(G.nodes[nodelist[3]]))\n",
    "print(G.nodes[nodelist[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from networkx.algorithms import community \n",
    "\n",
    "sorted_degree = sorted(degree_dict.items(), key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 20 nodes by degree:\")\n",
    "for d in sorted_degree[:20]:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betweenness_dict = nx.betweenness_centrality(G) # Run betweenness centrality\n",
    "eigenvector_dict = nx.eigenvector_centrality(G) # Run eigenvector centrality\n",
    "\n",
    "# Assign each to an attribute in your network\n",
    "nx.set_node_attributes(G, betweenness_dict, 'betweenness')\n",
    "nx.set_node_attributes(G, eigenvector_dict, 'eigenvector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_betweenness = sorted(betweenness_dict.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "print(\"Top 20 nodes by betweenness centrality:\")\n",
    "for b in sorted_betweenness[:20]:\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First get the top 20 nodes by betweenness as a list\n",
    "top_betweenness = sorted_betweenness[:20]\n",
    "\n",
    "#Then find and print their degree\n",
    "for tb in top_betweenness: # Loop through top_betweenness\n",
    "    degree = degree_dict[tb[0]] # Use degree_dict to access a node's degree, see footnote 2\n",
    "    print(\"Name:\", tb[0], \"| Betweenness Centrality:\", tb[1], \"| Degree:\", degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = nx.Graph(G) # need to convert to undirected graph to use greedy_modularity\n",
    "\n",
    "communities = community.greedy_modularity_communities(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modularity_dict = {} # Create a blank dictionary\n",
    "for i,c in enumerate(communities): # Loop through the list of communities, keeping track of the number for the community\n",
    "    for name in c: # Loop through each person in a community\n",
    "        modularity_dict[name] = i # Create an entry in the dictionary for the person, where the value is which group they belong to.\n",
    "\n",
    "# Now you can add modularity information like we did the other metrics\n",
    "nx.set_node_attributes(G, modularity_dict, 'modularity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_community_dict = pd.DataFrame(list(modularity_dict.items()),columns = ['screen_name','class_id']) \n",
    "class_list = df_community_dict.class_id.unique()\n",
    "\n",
    "print(\"Number of different communities = {} \".format(len(class_list)))\n",
    "\n",
    "df_community_dict_agg = df_community_dict.groupby('class_id').count()\n",
    "\n",
    "comm_count = 10\n",
    "print(\"\\nTop {} communities, by number of members\".format(comm_count))\n",
    "df_community_dict_agg.head(comm_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def print_class_members(class_num, count):\n",
    "\n",
    "    # First get a list of just the nodes in that class\n",
    "    class_id = [n for n in G.nodes() if G.nodes[n]['modularity'] == class_num]\n",
    "\n",
    "    # Then create a dictionary of the eigenvector centralities of those nodes\n",
    "    class_eigenvector = {n:G.nodes[n]['eigenvector'] for n in class_id}\n",
    "\n",
    "    # Then sort that dictionary and print the first 5 results\n",
    "    class_sorted_by_eigenvector = sorted(class_eigenvector.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "    print(\"\\n<--- Modularity Class {} Sorted by Eigenvector Centrality, top {} values --->\".format(class_num, count))\n",
    "    for node in class_sorted_by_eigenvector[:count]:\n",
    "        print(\"Name:\", node[0], \"| Eigenvector Centrality:\", node[1])\n",
    "        \n",
    "    my_subgraph = G.subgraph(class_id)\n",
    "    \n",
    "    plt.figure()\n",
    "    nx.draw(my_subgraph)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with classes allocated via labeled tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_users = pd.read_csv('./DataSources/TwitterData/labeled_users.csv')\n",
    "print(labeled_users.shape)\n",
    "print(labeled_users[labeled_users.tweeter_label=='AGAINST'].shape)\n",
    "labeled_users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_community_dict.shape)\n",
    "df_community_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(labeled_users, df_community_dict, left_on='tweeter_screen_name', right_on='screen_name')\n",
    "print(merged_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_against = merged_df[merged_df.tweeter_label=='AGAINST'].copy()\n",
    "against_count = just_against.shape[0]\n",
    "print(just_against.shape)\n",
    "just_against.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_against_sorted = just_against[['AGAINST','class_id']].groupby(['class_id'])['AGAINST'] \\\n",
    "                             .count() \\\n",
    "                             .reset_index(name='count') \\\n",
    "                             .sort_values(['count'], ascending=False) \n",
    "\n",
    "just_against_sorted['class_pct_of_total'] = just_against_sorted['count'] / against_count\n",
    "\n",
    "just_against_sorted.reset_index().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "We have 5,011 in combined dataframe and approximately 65% of these fall into the classes: 0, 1, 9, 5, 2, 13, 4 and 11\n",
    "\n",
    "Now we take a look at these classes to see their membership and whether they correlate.\n",
    "\n",
    "n.b. group by logic above courtesy of https://stackoverflow.com/questions/40454030/count-and-sort-with-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = [0, 1, 9, 5, 2, 13, 4, 11]\n",
    "top_member_count = 10\n",
    "\n",
    "for class_id in class_list:\n",
    "    print(\"<----- class {} ----->\".format(class_id))\n",
    "    print_class_members(class_id, top_member_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_community_just_against = df_community_dict[df_community_dict.class_id.isin(class_list)].copy()\n",
    "\n",
    "df_community_just_against_sorted = df_community_just_against.groupby('class_id').count().reset_index()\n",
    "df_community_just_against_sorted.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_merged = pd.merge(df_community_just_against_sorted, just_against_sorted, left_on='class_id', right_on='class_id')\n",
    "sorted_merged['pct_of_class'] = sorted_merged['count'] / sorted_merged['screen_name']\n",
    "\n",
    "sorted_merged.rename(columns = {'screen_name':'community_count', \n",
    "                                'count':'label_count', \n",
    "                                'class_pct_of_total':'class_as_pct_all_against_labels',\n",
    "                                'pct_of_class':'label_as_pct_whole_class'}, inplace = True)\n",
    "\n",
    "sorted_merged.sort_values(by=['label_as_pct_whole_class'], ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also need to see what proportion of NOT_AGAINST fall into these classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## carry on with bovet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edges are contained in a EdgeView with a set-like interface\n",
    "print(type(G.edges))\n",
    "print(G.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see all the edges going out of this node\n",
    "# each edge is a dictionary inside this dictionary with a key \n",
    "# corresponding to the target user_id\n",
    "e = G.out_edges(nodelist[4], data=True)\n",
    "print(nodelist[4])\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can iterate over the out-edges \n",
    "for s,t,data in e:\n",
    "    print(s,t,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary with the degree of all nodes\n",
    "all_degrees = [G.degree(n) for n in nodelist] # this is the degree for undirected edges\n",
    "in_degrees = [G.in_degree(n) for n in nodelist]\n",
    "out_degrees = [G.out_degree(n) for n in nodelist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average degree\n",
    "2*G.number_of_edges()/G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.array(all_degrees).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(in_degrees).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(out_degrees).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to make a list with (user_id, username, degree) for all nodes\n",
    "degree_node_list = []\n",
    "for node in nodelist:\n",
    "    degree_node_list.append((node, G.nodes[node]['name'], G.degree(node)))\n",
    "    \n",
    "print('Unordered user, degree list')    \n",
    "print(degree_node_list[:10])\n",
    "\n",
    "# sort the list according the degree in descinding order\n",
    "degree_node_list = sorted(degree_node_list, key=lambda x:x[2], reverse=True)\n",
    "print('Ordered user, degree list')    \n",
    "print(degree_node_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to import matplolib for making plots\n",
    "# and numpy for numerical computations\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network components\n",
    "Connected components are a subset of nodes in which each node has a minimum of one link with another node in the same subset, and any node that is a member of 'this' subset is not linked to any nodes external to the subset. \n",
    "\n",
    "In addition, for <b> directed </b> graphs we can define two types of connected components:\n",
    "- Weakly connected components, (WCC): maximal set of nodes where there exists a path in at least one direction between each pair of nodes.\n",
    "- Strongly connected components, (SCC): maximal set of nodes where there exists a path in both directions between each pair of nodes.\n",
    "\n",
    "And, within these categories we can additionally identify the weakly connected giant (WCGC), which is the largest of the weakly connected components and the strongly connected giant (SCGC), which is the largest of the strongly connected components. In effect these are the largest subgraphs within the network. Bovet et al illustrate this (incorrectly, for SCGC given arrows aren't two way) as follows:\n",
    "\n",
    "<img src=\"WCGC-SCGC.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strongly connected components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this returns a list of set of nodes belonging to the \n",
    "# different (strongly) connected components\n",
    "components_strong = list(nx.strongly_connected_components(G))\n",
    "\n",
    "# sort the component according to their size\n",
    "components_strong = list(sorted(components_strong, key=lambda x:len(x), reverse=True))\n",
    "\n",
    "# make a list with the size of each component\n",
    "comp_sizes_strong = []\n",
    "for comp in components_strong:\n",
    "    comp_sizes_strong.append(len(comp))\n",
    "    \n",
    "print(\"number of strong components: {}\".format(len(comp_sizes_strong)))\n",
    "\n",
    "print(\"sizes of the ten largest components: {}\".format(comp_sizes_strong[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_strongly_connected(idx):\n",
    "\n",
    "    largest_comp_strong = components_strong[idx]\n",
    "    LCC_strong = G.subgraph(largest_comp_strong)\n",
    "\n",
    "    print(\"number of components in LCC[{}] = {}\".format(idx, LCC_strong.number_of_nodes()))\n",
    "\n",
    "    # let's plot the degree distribution inside the LCC\n",
    "    degrees = [LCC_strong.degree(n) for n in LCC_strong.nodes()]\n",
    "    print(\"number of degrees for each node within LCC[{}] = {}\\n\".format(idx, degrees))\n",
    "    \n",
    "    # add weights to the edges based on how many times they are traversed\n",
    "    print(\"Weight of edges between each node:\\n\")\n",
    "    \n",
    "    for u, v, d in LCC_strong.edges(data=True):\n",
    "        d['weight'] = 1\n",
    "    for u,v,d in LCC_strong.edges(data=True):\n",
    "        print (u,v,d)\n",
    "    \n",
    "    ## <--- now look at centrality\n",
    "    betweenValues = nx.betweenness_centrality(LCC_strong)\n",
    "    \n",
    "    # betweenValues is a dictionary, let's get the values and keys in separate lists\n",
    "    values = list(betweenValues.values())\n",
    "    keys = list(betweenValues.keys())\n",
    "    \n",
    "    # find the index of the node with highest betweeness centrality\n",
    "    highestIndex = np.argmax(values)\n",
    "    \n",
    "    #Uses solution in the documentation: https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.algorithms.components.connected.connected_component_subgraphs.html\n",
    "    \n",
    "    print('\\n <---- centrality and clustering measures -------> \\n')\n",
    "    print(\"The node id \", keys[highestIndex], \" has the centrality degree of \", values[highestIndex])\n",
    "\n",
    "    overallAverage = []\n",
    "    # now on to looking at clustering coefficients\n",
    "    for i in range(0, countComps):\n",
    "        clustCoeff = nx.clustering( connectedSubgraphs[i])\n",
    "        coeffVals = list(clustCoeff.values())\n",
    "        overallAverage.append(np.average(coeffVals))\n",
    "    \n",
    "    print (\"Average clustering coefficent is: \", np.average(overallAverage))\n",
    "\n",
    "    ## <--- end of this\n",
    "    \n",
    "    \n",
    "    plt.figure()\n",
    "    nx.draw_networkx(LCC_strong)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "range_limit = 5\n",
    "for n in range(range_limit):\n",
    "    draw_strongly_connected(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## end of strongly connected components\n",
    "- need to comment on interesting not necessarily for the size of the strong components but mostly because of the prominence of their members in other communities - look at that as part of the weakly connected components plus also the community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this returns a list of set of nodes belonging to the \n",
    "# different (weakly) connected components\n",
    "components = list(nx.weakly_connected_components(G))\n",
    "\n",
    "# sort the component according to their size\n",
    "components = list(sorted(components, key=lambda x:len(x), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list with the size of each component\n",
    "comp_sizes = []\n",
    "for comp in components:\n",
    "    comp_sizes.append(len(comp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histogram of component sizes\n",
    "hist = plt.hist(comp_sizes, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram with logarithmic y scale\n",
    "hist = plt.hist(comp_sizes, bins=100, log=True)\n",
    "tx = plt.xlabel('component size')\n",
    "ty = plt.ylabel('number of components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sizes of the ten largest components\n",
    "comp_sizes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make a new graph which is the subgraph of G corresponding to \n",
    "# the largest connected component\n",
    "# let's find the largest component\n",
    "largest_comp = components[2]\n",
    "LCC = G.subgraph(largest_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCC.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the degree distribution inside the LCC\n",
    "degrees = [LCC.degree(n) for n in LCC.nodes()]\n",
    "degrees.sort(reverse=True)\n",
    "degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "nx.draw_networkx(LCC)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "degree_array = np.array(degrees)\n",
    "hist = plt.hist(degree_array, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# using logarithmic scales\n",
    "hist = plt.hist(degree_array, bins=100, log=True)\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logarithmic scale with logarithmic bins\n",
    "N, bins, patches = plt.hist(degree_array, bins=np.logspace(0,np.log10(degree_array.max()+1), 20), log=True)\n",
    "plt.xscale('log')\n",
    "tx = plt.xlabel('k - degree')\n",
    "ty= plt.ylabel('number of nodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree probability distribution (P(k))\n",
    "\n",
    "# since we have logarithmic bins, we need to\n",
    "# take into account the fact that the bins \n",
    "# have different lenghts when normalizing\n",
    "bin_lengths = np.diff(bins) # lenght of each bin\n",
    "\n",
    "summ = np.sum(N*bin_lengths)\n",
    "normalized_degree_dist = N/summ\n",
    "\n",
    "# check normalization:\n",
    "print(np.sum(normalized_degree_dist*bin_lengths))\n",
    "\n",
    "hist = plt.bar(bins[:-1], normalized_degree_dist, width=np.diff(bins))\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "tx = plt.xlabel('k (degree)')\n",
    "ty = plt.ylabel('P(k)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find most important Tweeters using Page Rank\n",
    "PageRank is a generalisation of Google's websearch algorithm, which was originally a method for returning the most important web pages for a given search term. It defined importance as the page which - need to talk about teleporting and surfer stuff - reference this bloke: Gleich, D.F., 2015. PageRank beyond the web. Siam Review, 57(3), pp.321-363.\n",
    "\n",
    "Also then look at from the perspective of centrality as this tells us which nodes(pages) have most edges in or out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#teleportation probability\n",
    "alpha = 0.15\n",
    "\n",
    "#adjacency matrix\n",
    "nodelist = list(G.nodes())\n",
    "A = nx.to_numpy_array(G, nodelist=nodelist)\n",
    "\n",
    "#diagonal matrix of out degrees\n",
    "deg_out_vect = np.array([float(max(G.out_degree(n),1)) for n in nodelist])\n",
    "D_out_inv = np.diag(1/deg_out_vect)\n",
    "\n",
    "# teleportation transition matrix\n",
    "N = A.shape[1]\n",
    "S = np.ones((N,N))*1/N\n",
    "\n",
    "# full transition matrix\n",
    "M = (1-alpha)*D_out_inv @ A + alpha*S\n",
    "\n",
    "# for dangling nodes (nodes without out-edges), we force the random teleportation\n",
    "dangling_nodes = np.where(A.sum(1) == 0)[0]\n",
    "M[dangling_nodes,:] = S[dangling_nodes,:]\n",
    "\n",
    "#initial walker distribution and 1st iteration\n",
    "p_last = np.ones(N)*1/N\n",
    "p = np.matmul(p_last, M)\n",
    "\n",
    "# iterate until sufficient convergence\n",
    "eps = 1.0e-8\n",
    "i = 1\n",
    "while np.linalg.norm(p - p_last, 2) > eps:\n",
    "        p_last = p\n",
    "        p = np.matmul(p, M)\n",
    "        i += 1\n",
    "\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_ranking = np.array(np.argsort(p)[::-1])\n",
    "\n",
    "pagerank_values = p[pg_ranking]\n",
    "nodes_pagerank = [nodelist[r] for r in pg_ranking]\n",
    "nodes_pagerank[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_pagerank = [G.nodes[n]['name'] for n in nodes_pagerank]\n",
    "names_pagerank[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare page rank with degree\n",
    "Top 20 nodes by degree:\n",
    "- ('KoolKat1025', 911)\n",
    "- ('LeoKearse', 629)\n",
    "- ('LeaveEUOfficial', 537)\n",
    "- ('SadiqKhan', 510)\n",
    "- ('MayorofLondon', 376)\n",
    "- ('BrexitBassist', 317)\n",
    "- ('PoliticsJOE_UK', 314)\n",
    "- ('mariannaspring', 294)\n",
    "- ('PoliticsForAlI', 240)\n",
    "- ('LBC', 235)\n",
    "- ('PrisonPlanet', 230)\n",
    "- ('NKrankie', 219)\n",
    "- ('talkRADIO', 210)\n",
    "- ('TJ_Knight', 192)\n",
    "- ('metpoliceuk', 171)\n",
    "- ('ashindestad', 170)\n",
    "- ('MickeyD44314901', 160)\n",
    "- ('DJBURNS_was', 160)\n",
    "- ('Independent', 160)\n",
    "- ('standardnews', 152)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = plt.bar(np.arange(p.shape[0]),np.sort(p)[::-1])\n",
    "ty = plt.ylabel('PageRank value')\n",
    "tx = plt.xlabel('PageRank ranking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pagerank is a probability density\n",
    "pagerank_values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the network of the top 5 nodes\n",
    "plt.figure(figsize=(10,10))\n",
    "nx.draw(G, nodelist=nodes_pagerank[:5], node_size=8000*pagerank_values[:5],width=0.5, arrows=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's add the pagerank value as a node attribute\n",
    "for n, pr in zip(nodes_pagerank,pagerank_values):\n",
    "    if n in LCC:\n",
    "        LCC.nodes[n]['page_rank'] = pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_pagerank[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
