{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Tweets\n",
    "## Tools\n",
    "We set up a Twitter developer account and attempted to use the Tweepy tool to extract tweets. Our method was influenced by \n",
    "https://www.earthdatascience.org/courses/use-data-open-source-python/intro-to-apis/twitter-data-in-python/\n",
    "\n",
    "We did extract data but we encountered problems when trying to access historic data.\n",
    "\n",
    "## Problems encountered\n",
    "\n",
    "Unfortunately, the Twitter API only returns tweets from the most recent week, irrespective of which start date one provides. This means that we will have to run the extract on a weekly basis (to get the prior week's tweets) in order to get a reasonable body of tweets. This is suboptimal and could restrict our ability to go back and query prior results.\n",
    "\n",
    "I addition, running search queries on Twitter takes a lot of time and can get timed out. For example, see Appendix A for messages received when I timed out searching Twitter using the search term \"sadiq AND khan\". Previously this terms had taken 3 hours to complete and returned approximately 12,000 rows. However, it has then consistently failed due to timeouts. I am not sure whether this is happenstance or whether Twitter have throttled my ability to download large volumes of Twitter data, given I had recently downloaded this data. This is of interest to other researchers using developer accounts. \n",
    "\n",
    "## Twitter research account\n",
    "I therefore applied to Twitter's Academic Research track https://developer.twitter.com/en/products/twitter-api/academic-research because this allows researchers to access historic tweets, and in higher tweet volumes. My requests were rejected and I discuss this in further detail in Appendix B. The reason I discuss this is because researching Twitter data without academic access is difficult and yet getting academic access whilst not having a presence on University websites does not seem possible \n",
    "\n",
    "## Tweepy Code References:\n",
    "- retweet and favourite counts, better dataframe creator using Tweepy - https://towardsdatascience.com/how-to-build-a-dataset-from-twitter-using-python-tweepy-861bdbc16fa5\n",
    "- Getting user and location - https://stackoverflow.com/questions/50366489/how-to-get-twitter-users-screen-name-or-userid-from-a-specific-geolocations\n",
    "- Cleaning tweet text and finding out if retweet - https://stackoverflow.com/questions/50052330/tweepy-check-if-a-tweet-is-a-retweet\n",
    "- geocordinates - https://stackoverflow.com/questions/46044445/not-able-to-scrape-geo-coordinate-with-tweets-lat-lon\n",
    "- avoiding twitter api rate limit - https://stackoverflow.com/questions/21308762/avoid-twitter-api-limitation-with-tweepy\n",
    "- keeping authentication details secret - https://www.digitalocean.com/community/tutorials/how-to-create-a-twitterbot-with-python-3-and-the-tweepy-library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tweepy\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get Twitter Data\n",
    "We have two choices to loading twitter data:\n",
    "- 1.1. use the Tweepy API (but this can take hours)\n",
    "- 1.2. load the previously saved Twitter data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load data from Twitter \n",
    "#### 1.1.1 Twitter credentials file\n",
    "I don't want to make my Twitter credentials public and so these are loaded from a credentials file and that file is not uploaded to github. \n",
    "\n",
    "To replicate this code, create a 'credentials.py' file with the following lines (using your own credential details):\n",
    "\n",
    "`\n",
    "consumer_key = 'your_consumer_key'\n",
    "consumer_secret = 'your_consumer_secret'\n",
    "access_token = 'your_access_token'\n",
    "access_token_secret = 'your_access_token_secret'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from credentials import *\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1.1.2 Set date parameters\n",
    "Using search words, I want to get all tweets between today's date and a start date of July 1, 2019\n",
    "- The start date is just before the day the Mayor made his speech and today's date is used so I can collect as many tweets as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./DataSources/TwitterData/raw_tweets_20210726.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(datetime.date(2019, 7, 1), '20210726')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_from = datetime.date(2019, 7, 1) # this doesn't actually work as twitter only goes back one week\n",
    "today = datetime.datetime.today().strftime('%Y%m%d')\n",
    "\n",
    "outputfile_str = \"./DataSources/TwitterData/raw_tweets_\" + today + \".csv\"\n",
    "print(outputfile_str)\n",
    "\n",
    "date_from, today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 Get tweets using a cursor\n",
    "- First we define our function to load tweets\n",
    "- Create search terms to query Twitter - return results as a list of dictionary items\n",
    "- Concatenate all returned results and then use this to create a pandas dataframe\n",
    "\n",
    "##### 1.1.3.1 define get_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_tweets(search_words, my_api, today): \n",
    "    tic = time.perf_counter()\n",
    "    tweets = tweepy.Cursor(my_api.search,\n",
    "                       q=search_words,\n",
    "                       lang=\"en\",\n",
    "                       since=date_from).items()\n",
    "    \n",
    "    output = []\n",
    "    for tweet in tweets:\n",
    "        tweet_id = tweet.id\n",
    "        text = tweet.text\n",
    "        tweet_date = tweet.created_at\n",
    "        user_id_str = tweet.user.id_str\n",
    "        screen_name = tweet.user.screen_name\n",
    "        user_name = tweet.user.name\n",
    "        user_id = api.get_user(user_id_str)\n",
    "        user_location = user_id.location\n",
    "        user_coordinates = tweet.coordinates\n",
    "        favourite_count = tweet.favorite_count\n",
    "        retweet_count = tweet.retweet_count\n",
    "                \n",
    "        line = {'tweet_id' : tweet_id,\n",
    "                'tweet_date' : tweet_date,\n",
    "                'tweeter_id' : user_id_str,\n",
    "                'tweeter_user_name' : user_name,\n",
    "                'tweeter_screen_name' : screen_name,\n",
    "                'tweeter_location' : user_location,\n",
    "                'tweeter_coordinates' : user_coordinates,\n",
    "                'message_text' : text,\n",
    "                'favourite_count' : favourite_count, \n",
    "                'retweet_count' : retweet_count,\n",
    "                'extract_run_date' : today,\n",
    "                'retrieved_using_search_term' : search_words}\n",
    "        output.append(line)\n",
    "        \n",
    "        \n",
    "    toc = time.perf_counter()\n",
    "    time_taken = toc - tic\n",
    "    \n",
    "    print('Time taken to process search term : {} , was {:.2f}'.format(search_words, time_taken))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.3.2 create list of search terms and iteratively get tweets using these terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to process search term : London AND knife AND crime , was 910.26\n",
      "Time taken to process search term : London AND knifecrime , was 53.89\n",
      "Time taken to process search term : Khan AND knife AND crime , was 94.11\n",
      "Time taken to process search term : Khan AND knifecrime , was 1.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to process search term : London AND violent AND crime , was 816.40\n",
      "Time taken to process search term : youth AND violent AND crime , was 18.18\n",
      "Time taken to process search term : youth AND crime AND Londonyouth AND knife AND crime , was 0.22\n",
      "Time taken to process search term : london AND youthcrime , was 0.21\n",
      "Time taken to process search term : #knifecrime AND #khan , was 1.17\n",
      "Time taken to process search term : #knifecrime AND #london , was 59.87\n",
      "Time taken to process search term : #violence AND #khan , was 0.23\n",
      "Time taken to process search term : #london AND #youthcrime , was 0.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 650\n",
      "Rate limit reached. Sleeping for: 641\n",
      "Rate limit reached. Sleeping for: 638\n",
      "Rate limit reached. Sleeping for: 664\n",
      "Rate limit reached. Sleeping for: 656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to process search term : London AND crime , was 4607.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to process search term : London AND stabbing , was 873.63\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'search_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-b16c6433efb3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mtime_taken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoc\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Time taken to process ALL search terms : {} , was {:.2f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'search_words' is not defined"
     ]
    }
   ],
   "source": [
    "search_terms = [\"London AND knife AND crime\",\n",
    "                \"London AND knifecrime\",\n",
    "                \"Khan AND knife AND crime\",\n",
    "                \"Khan AND knifecrime\",\n",
    "                \"London AND violent AND crime\",\n",
    "                \"youth AND violent AND crime\",\n",
    "                \"youth AND crime AND London\"\n",
    "                \"youth AND knife AND crime\",\n",
    "                \"london AND youthcrime\",\n",
    "                \"#knifecrime AND #khan\",\n",
    "                \"#knifecrime AND #london\",\n",
    "                \"#violence AND #khan\",\n",
    "                \"#london AND #youthcrime\",\n",
    "                \"London AND crime\",\n",
    "                \"London AND stabbing\"]\n",
    "\n",
    "# The following term times out\n",
    "# search_terms = [\"sadiq AND khan\"]\n",
    "\n",
    "all_tweets = []\n",
    "\n",
    "tic = time.perf_counter()\n",
    "\n",
    "for search_term in search_terms:\n",
    "    current_tweets = get_tweets(search_term, api, today)\n",
    "    all_tweets.append(current_tweets)\n",
    "\n",
    "toc = time.perf_counter()\n",
    "time_taken = toc - tic\n",
    "    \n",
    "print('Time taken to process ALL search terms : {} , was {:.2f}'.format(search_terms, time_taken))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4 create all_tweets_df dataframe\n",
    "First check on how many items downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tweets in current list = 920\n",
      "number of tweets in current list = 207\n",
      "number of tweets in current list = 361\n",
      "number of tweets in current list = 5\n",
      "number of tweets in current list = 555\n",
      "number of tweets in current list = 66\n",
      "number of tweets in current list = 0\n",
      "number of tweets in current list = 0\n",
      "number of tweets in current list = 2\n",
      "number of tweets in current list = 197\n",
      "number of tweets in current list = 0\n",
      "number of tweets in current list = 0\n",
      "number of tweets in current list = 4832\n",
      "number of tweets in current list = 786\n",
      "(7931, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>tweeter_id</th>\n",
       "      <th>tweeter_user_name</th>\n",
       "      <th>tweeter_screen_name</th>\n",
       "      <th>tweeter_location</th>\n",
       "      <th>tweeter_coordinates</th>\n",
       "      <th>message_text</th>\n",
       "      <th>favourite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>extract_run_date</th>\n",
       "      <th>retrieved_using_search_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1419565076676231170</td>\n",
       "      <td>2021-07-26 07:47:43</td>\n",
       "      <td>1416666869755428867</td>\n",
       "      <td>You've been Webber'd</td>\n",
       "      <td>ve_webber</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>RT @ve_webber: My podcast with @Nina_c\\nAbout ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>20210726</td>\n",
       "      <td>London AND knife AND crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1419560908490149890</td>\n",
       "      <td>2021-07-26 07:31:09</td>\n",
       "      <td>851474124</td>\n",
       "      <td>Paul Cater</td>\n",
       "      <td>cater_paul</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>None</td>\n",
       "      <td>RT @MPFed: 'I've stopped kids as young as 10 w...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20210726</td>\n",
       "      <td>London AND knife AND crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1419560683562168320</td>\n",
       "      <td>2021-07-26 07:30:15</td>\n",
       "      <td>214120999</td>\n",
       "      <td>Met Police Federation</td>\n",
       "      <td>MPFed</td>\n",
       "      <td>London, UK</td>\n",
       "      <td>None</td>\n",
       "      <td>'I've stopped kids as young as 10 with knives'...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>20210726</td>\n",
       "      <td>London AND knife AND crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1419557144064626690</td>\n",
       "      <td>2021-07-26 07:16:11</td>\n",
       "      <td>881846230279749632</td>\n",
       "      <td>Bear Pitt</td>\n",
       "      <td>bearpittelder</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>RT @enuffsa1d: Welcome to @SadiqKhan London. O...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>20210726</td>\n",
       "      <td>London AND knife AND crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1419555847332503552</td>\n",
       "      <td>2021-07-26 07:11:02</td>\n",
       "      <td>1079370138141556737</td>\n",
       "      <td>Bilbo Biggin</td>\n",
       "      <td>biggin_bilbo</td>\n",
       "      <td>England, United Kingdom</td>\n",
       "      <td>None</td>\n",
       "      <td>RT @enuffsa1d: Welcome to @SadiqKhan London. O...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>20210726</td>\n",
       "      <td>London AND knife AND crime</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id          tweet_date           tweeter_id  \\\n",
       "0  1419565076676231170 2021-07-26 07:47:43  1416666869755428867   \n",
       "1  1419560908490149890 2021-07-26 07:31:09            851474124   \n",
       "2  1419560683562168320 2021-07-26 07:30:15            214120999   \n",
       "3  1419557144064626690 2021-07-26 07:16:11   881846230279749632   \n",
       "4  1419555847332503552 2021-07-26 07:11:02  1079370138141556737   \n",
       "\n",
       "       tweeter_user_name tweeter_screen_name         tweeter_location  \\\n",
       "0   You've been Webber'd           ve_webber                            \n",
       "1             Paul Cater          cater_paul           United Kingdom   \n",
       "2  Met Police Federation               MPFed               London, UK   \n",
       "3              Bear Pitt       bearpittelder                            \n",
       "4           Bilbo Biggin        biggin_bilbo  England, United Kingdom   \n",
       "\n",
       "  tweeter_coordinates                                       message_text  \\\n",
       "0                None  RT @ve_webber: My podcast with @Nina_c\\nAbout ...   \n",
       "1                None  RT @MPFed: 'I've stopped kids as young as 10 w...   \n",
       "2                None  'I've stopped kids as young as 10 with knives'...   \n",
       "3                None  RT @enuffsa1d: Welcome to @SadiqKhan London. O...   \n",
       "4                None  RT @enuffsa1d: Welcome to @SadiqKhan London. O...   \n",
       "\n",
       "  favourite_count retweet_count extract_run_date retrieved_using_search_term  \n",
       "0               0             2         20210726  London AND knife AND crime  \n",
       "1               0             1         20210726  London AND knife AND crime  \n",
       "2               5             1         20210726  London AND knife AND crime  \n",
       "3               0             8         20210726  London AND knife AND crime  \n",
       "4               0             8         20210726  London AND knife AND crime  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets_df = pd.DataFrame(columns=['tweet_id', \n",
    "                                      'tweet_date', \n",
    "                                      'tweeter_id', \n",
    "                                      'tweeter_user_name', \n",
    "                                      'tweeter_screen_name', \n",
    "                                      'tweeter_location',\n",
    "                                      'tweeter_coordinates',\n",
    "                                      'message_text',\n",
    "                                      'favourite_count',\n",
    "                                      'retweet_count',\n",
    "                                      'extract_run_date',\n",
    "                                      'retrieved_using_search_term'])\n",
    "\n",
    "for these_tweets in all_tweets:\n",
    "    print('number of tweets in current list = {}'.format(len(these_tweets)))\n",
    "\n",
    "    df_tweets = pd.DataFrame(these_tweets)\n",
    "    all_tweets_df = all_tweets_df.append(df_tweets, ignore_index=True)\n",
    "\n",
    "print(all_tweets_df.shape)\n",
    "\n",
    "all_tweets_df.to_csv(outputfile_str, index=False)\n",
    "\n",
    "all_tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Load previously saved Twitter data\n",
    "- Need to change the file name passed to 'load_file_name' if we want a prior dataset\n",
    "- Will eventually concatenate all the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7931, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>tweeter_id</th>\n",
       "      <th>tweeter_user_name</th>\n",
       "      <th>tweeter_screen_name</th>\n",
       "      <th>tweeter_location</th>\n",
       "      <th>tweeter_coordinates</th>\n",
       "      <th>message_text</th>\n",
       "      <th>favourite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>extract_run_date</th>\n",
       "      <th>retrieved_using_search_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1419565076676231170</td>\n",
       "      <td>2021-07-26 07:47:43</td>\n",
       "      <td>1416666869755428867</td>\n",
       "      <td>You've been Webber'd</td>\n",
       "      <td>ve_webber</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @ve_webber: My podcast with @Nina_c\\nAbout ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>20210726</td>\n",
       "      <td>London AND knife AND crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1419560908490149890</td>\n",
       "      <td>2021-07-26 07:31:09</td>\n",
       "      <td>851474124</td>\n",
       "      <td>Paul Cater</td>\n",
       "      <td>cater_paul</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @MPFed: 'I've stopped kids as young as 10 w...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20210726</td>\n",
       "      <td>London AND knife AND crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1419560683562168320</td>\n",
       "      <td>2021-07-26 07:30:15</td>\n",
       "      <td>214120999</td>\n",
       "      <td>Met Police Federation</td>\n",
       "      <td>MPFed</td>\n",
       "      <td>London, UK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>'I've stopped kids as young as 10 with knives'...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>20210726</td>\n",
       "      <td>London AND knife AND crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1419557144064626690</td>\n",
       "      <td>2021-07-26 07:16:11</td>\n",
       "      <td>881846230279749632</td>\n",
       "      <td>Bear Pitt</td>\n",
       "      <td>bearpittelder</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @enuffsa1d: Welcome to @SadiqKhan London. O...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>20210726</td>\n",
       "      <td>London AND knife AND crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1419555847332503552</td>\n",
       "      <td>2021-07-26 07:11:02</td>\n",
       "      <td>1079370138141556737</td>\n",
       "      <td>Bilbo Biggin</td>\n",
       "      <td>biggin_bilbo</td>\n",
       "      <td>England, United Kingdom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @enuffsa1d: Welcome to @SadiqKhan London. O...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>20210726</td>\n",
       "      <td>London AND knife AND crime</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id           tweet_date           tweeter_id  \\\n",
       "0  1419565076676231170  2021-07-26 07:47:43  1416666869755428867   \n",
       "1  1419560908490149890  2021-07-26 07:31:09            851474124   \n",
       "2  1419560683562168320  2021-07-26 07:30:15            214120999   \n",
       "3  1419557144064626690  2021-07-26 07:16:11   881846230279749632   \n",
       "4  1419555847332503552  2021-07-26 07:11:02  1079370138141556737   \n",
       "\n",
       "       tweeter_user_name tweeter_screen_name         tweeter_location  \\\n",
       "0   You've been Webber'd           ve_webber                      NaN   \n",
       "1             Paul Cater          cater_paul           United Kingdom   \n",
       "2  Met Police Federation               MPFed               London, UK   \n",
       "3              Bear Pitt       bearpittelder                      NaN   \n",
       "4           Bilbo Biggin        biggin_bilbo  England, United Kingdom   \n",
       "\n",
       "  tweeter_coordinates                                       message_text  \\\n",
       "0                 NaN  RT @ve_webber: My podcast with @Nina_c\\nAbout ...   \n",
       "1                 NaN  RT @MPFed: 'I've stopped kids as young as 10 w...   \n",
       "2                 NaN  'I've stopped kids as young as 10 with knives'...   \n",
       "3                 NaN  RT @enuffsa1d: Welcome to @SadiqKhan London. O...   \n",
       "4                 NaN  RT @enuffsa1d: Welcome to @SadiqKhan London. O...   \n",
       "\n",
       "   favourite_count  retweet_count  extract_run_date  \\\n",
       "0                0              2          20210726   \n",
       "1                0              1          20210726   \n",
       "2                5              1          20210726   \n",
       "3                0              8          20210726   \n",
       "4                0              8          20210726   \n",
       "\n",
       "  retrieved_using_search_term  \n",
       "0  London AND knife AND crime  \n",
       "1  London AND knife AND crime  \n",
       "2  London AND knife AND crime  \n",
       "3  London AND knife AND crime  \n",
       "4  London AND knife AND crime  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_file_name = outputfile_str\n",
    "\n",
    "all_tweets_df_new = pd.read_csv(load_file_name)\n",
    "print(all_tweets_df_new.shape)\n",
    "all_tweets_df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A - Twitter Timeout error\n",
    "The following error message was received when extracting data using the search term \"Sadiq AND Khan\"\n",
    "\n",
    "### error message\n",
    "TimeoutError                              Traceback (most recent call last)\n",
    "~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py in _new_conn(self)\n",
    "    158         try:\n",
    "--> 159             conn = connection.create_connection(\n",
    "    160                 (self._dns_host, self.port), self.timeout, **extra_kw\n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py in create_connection(address, timeout, source_address, socket_options)\n",
    "     83     if err is not None:\n",
    "---> 84         raise err\n",
    "     85 \n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py in create_connection(address, timeout, source_address, socket_options)\n",
    "     73                 sock.bind(source_address)\n",
    "---> 74             sock.connect(sa)\n",
    "     75             return sock\n",
    "\n",
    "TimeoutError: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond\n",
    "\n",
    "During handling of the above exception, another exception occurred:\n",
    "\n",
    "NewConnectionError                        Traceback (most recent call last)\n",
    "~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\n",
    "    669             # Make the request on the httplib connection object.\n",
    "--> 670             httplib_response = self._make_request(\n",
    "    671                 conn,\n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)\n",
    "    380         try:\n",
    "--> 381             self._validate_conn(conn)\n",
    "    382         except (SocketTimeout, BaseSSLError) as e:\n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py in _validate_conn(self, conn)\n",
    "    975         if not getattr(conn, \"sock\", None):  # AppEngine might not have  `.sock`\n",
    "--> 976             conn.connect()\n",
    "    977 \n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py in connect(self)\n",
    "    307         # Add certificate verification\n",
    "--> 308         conn = self._new_conn()\n",
    "    309         hostname = self.host\n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py in _new_conn(self)\n",
    "    170         except SocketError as e:\n",
    "--> 171             raise NewConnectionError(\n",
    "    172                 self, \"Failed to establish a new connection: %s\" % e\n",
    "\n",
    "NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000183218E7B50>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond\n",
    "\n",
    "During handling of the above exception, another exception occurred:\n",
    "\n",
    "MaxRetryError                             Traceback (most recent call last)\n",
    "~\\anaconda3\\lib\\site-packages\\requests\\adapters.py in send(self, request, stream, timeout, verify, cert, proxies)\n",
    "    438             if not chunked:\n",
    "--> 439                 resp = conn.urlopen(\n",
    "    440                     method=request.method,\n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\n",
    "    723 \n",
    "--> 724             retries = retries.increment(\n",
    "    725                 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py in increment(self, method, url, response, error, _pool, _stacktrace)\n",
    "    438         if new_retry.is_exhausted():\n",
    "--> 439             raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
    "    440 \n",
    "\n",
    "MaxRetryError: HTTPSConnectionPool(host='api.twitter.com', port=443): Max retries exceeded with url: /1.1/users/show.json?id=862885892 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000183218E7B50>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond'))\n",
    "\n",
    "During handling of the above exception, another exception occurred:\n",
    "\n",
    "ConnectionError                           Traceback (most recent call last)\n",
    "~\\anaconda3\\lib\\site-packages\\tweepy\\binder.py in execute(self)\n",
    "    183                 try:\n",
    "--> 184                     resp = self.session.request(self.method,\n",
    "    185                                                 full_url,\n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\requests\\sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\n",
    "    529         send_kwargs.update(settings)\n",
    "--> 530         resp = self.send(prep, **send_kwargs)\n",
    "    531 \n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\requests\\sessions.py in send(self, request, **kwargs)\n",
    "    642         # Send the request\n",
    "--> 643         r = adapter.send(request, **kwargs)\n",
    "    644 \n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\requests\\adapters.py in send(self, request, stream, timeout, verify, cert, proxies)\n",
    "    515 \n",
    "--> 516             raise ConnectionError(e, request=request)\n",
    "    517 \n",
    "\n",
    "ConnectionError: HTTPSConnectionPool(host='api.twitter.com', port=443): Max retries exceeded with url: /1.1/users/show.json?id=862885892 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000183218E7B50>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond'))\n",
    "\n",
    "During handling of the above exception, another exception occurred:\n",
    "\n",
    "TweepError                                Traceback (most recent call last)\n",
    "<ipython-input-7-c24d3d2b3d48> in <module>\n",
    "     18 \n",
    "     19 for search_term in search_terms:\n",
    "---> 20     current_tweets = get_tweets(search_term, api, today)\n",
    "     21     all_tweets.append(current_tweets)\n",
    "     22 \n",
    "\n",
    "<ipython-input-4-92b12659b409> in get_tweets(search_words, my_api, today)\n",
    "     16         screen_name = tweet.user.screen_name\n",
    "     17         user_name = tweet.user.name\n",
    "---> 18         user_id = api.get_user(user_id_str)\n",
    "     19         user_location = user_id.location\n",
    "     20         user_coordinates = tweet.coordinates\n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\tweepy\\binder.py in _call(*args, **kwargs)\n",
    "    251                 return method\n",
    "    252             else:\n",
    "--> 253                 return method.execute()\n",
    "    254         finally:\n",
    "    255             method.session.close()\n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\tweepy\\binder.py in execute(self)\n",
    "    190                                                 proxies=self.api.proxy)\n",
    "    191                 except Exception as e:\n",
    "--> 192                     six.reraise(TweepError, TweepError('Failed to send request: %s' % e), sys.exc_info()[2])\n",
    "    193 \n",
    "    194                 rem_calls = resp.headers.get('x-rate-limit-remaining')\n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\six.py in reraise(tp, value, tb)\n",
    "    700                 value = tp()\n",
    "    701             if value.__traceback__ is not tb:\n",
    "--> 702                 raise value.with_traceback(tb)\n",
    "    703             raise value\n",
    "    704         finally:\n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\tweepy\\binder.py in execute(self)\n",
    "    182                 # Execute request\n",
    "    183                 try:\n",
    "--> 184                     resp = self.session.request(self.method,\n",
    "    185                                                 full_url,\n",
    "    186                                                 data=self.post_data,\n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\requests\\sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\n",
    "    528         }\n",
    "    529         send_kwargs.update(settings)\n",
    "--> 530         resp = self.send(prep, **send_kwargs)\n",
    "    531 \n",
    "    532         return resp\n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\requests\\sessions.py in send(self, request, **kwargs)\n",
    "    641 \n",
    "    642         # Send the request\n",
    "--> 643         r = adapter.send(request, **kwargs)\n",
    "    644 \n",
    "    645         # Total elapsed time of the request (approximately)\n",
    "\n",
    "~\\anaconda3\\lib\\site-packages\\requests\\adapters.py in send(self, request, stream, timeout, verify, cert, proxies)\n",
    "    514                 raise SSLError(e, request=request)\n",
    "    515 \n",
    "--> 516             raise ConnectionError(e, request=request)\n",
    "    517 \n",
    "    518         except ClosedPoolError as e:\n",
    "\n",
    "TweepError: Failed to send request: HTTPSConnectionPool(host='api.twitter.com', port=443): Max retries exceeded with url: /1.1/users/show.json?id=862885892 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000183218E7B50>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix B - Twitter academic research access\n",
    "As discussed, I applied for the research track as this offers access to historical twitter data and also the ability to download higher volumes of Tweets. Unfortunately my access requests were rejected.\n",
    "\n",
    "My original request failed, with Twitter responding that I did not meet their use case for a research account. I believe this is because they wanted to be able to reference my name via an official University website, for example within a Student directory. However, City University do not make student directories publicly available (for entirely understandable reasons), which suggests the research account isn't readily available to students and it is more aimed at researchers and faculty members who are referencable via the University website. I then reapplied using my city email address and having set up a Twitter account linked to this email address. I am waiting for a response (25/07/2021).\n",
    "- I received a response on 25/07/2021 saying my request did not \"qualify for academic access to the Twitter API\". Twitter do not give specific reasons and so it's not possible to understand whether they do not give access if they cannot identify students on a university directory or whether this specific research falls outside what they consider acceptable research (although they did say it qualified for regular developer access, which suggests the research topic was OK).\n",
    "\n",
    "I any case I go into detail on the application process because not having access to historic tweets significantly impacts our ability to perform the desired research and the process to get access is time consuming, opaque and there is no right to appeal. It would discourage me from doing further academic research into Twitter data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
